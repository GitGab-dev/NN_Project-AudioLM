{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e39015-58b0-459a-9d16-e558dfbaaf18",
   "metadata": {},
   "source": [
    "# Re-implementation of AudioLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac863704-847f-4414-a525-0ce70fb47f79",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi quis eros fringilla, tristique sem non, dapibus urna. Integer scelerisque porta nulla eget lobortis. Donec quis malesuada est. Phasellus porta risus non nulla egestas dapibus. Cras vehicula in sapien in tempus. Maecenas dui odio, congue eget aliquam eget, congue ut eros. Donec elementum pretium iaculis. Ut eu purus et tortor faucibus pulvinar. Sed feugiat bibendum metus a rhoncus.\n",
    "\n",
    "Pellentesque nunc dolor, tincidunt ut leo condimentum, fermentum dictum nisl. Pellentesque viverra diam sit amet augue gravida tristique. Integer varius finibus tortor in cursus. Donec faucibus tristique risus. Pellentesque mattis enim et odio pulvinar euismod. Sed dignissim dolor sit amet risus mattis, eget iaculis odio tincidunt. Morbi id commodo sem, quis euismod purus. Nunc volutpat vitae erat id dictum. Pellentesque ac tincidunt est. Sed in enim vitae felis tempor laoreet. Donec sit amet facilisis quam. Etiam vitae facilisis odio, nec malesuada massa. Praesent dignissim, leo cursus lacinia vehicula, diam ipsum iaculis velit, sed volutpat tellus risus non elit. Nunc non finibus nibh. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e88ef7-a623-4482-838e-ff9232d14cb6",
   "metadata": {},
   "source": [
    "## Import and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f59ecc-11cf-424a-ba34-3fa5d6f03ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d4a24-5231-4d13-89e8-2d54176bea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310502d-4418-4fb4-a335-8302d0a3caa0",
   "metadata": {},
   "source": [
    "## Semantic tokens with w2v-BERT-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25020f93-6094-41b7-9640-87c1f301577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hubertKM import SemanticTokenizer, visualizeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c836fbe-0044-4409-8627-4ecc024723da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vBERT = SemanticTokenizer(\"facebook/hubert-base-ls960\",\"./hubertKM/hubert_base_ls960_L9_km500.bin\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff722dd-6d36-4e41-8f80-98e04b4a49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f3ff2-9486-45cd-abf5-b83515e5f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce159db-7790-415b-a544-259d6c464d70",
   "metadata": {},
   "source": [
    "## Acoustic tokens with SoundStream codec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db3a12-9ce3-4add-8d5a-9554d0bba11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoundStream import soundstream_16khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379563a0-97f8-44a3-9f50-81636b85a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundStream = soundstream_16khz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63a8b6-972b-48ce-8334-a08ac053d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ecfa53-9a2b-4152-8ed2-cdf684567b18",
   "metadata": {},
   "source": [
    "## Token Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cd0fb-6096-46e7-ae44-78ce0e43f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import storeTokens, TokensDataset, store_from_librilight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0765fa3-fe7f-4ff1-83c4-0e3adba53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenPath = \"out\" ## output file directory\n",
    "tokenFile = \"tokens.csv\" ## output file name\n",
    "audioPath = \"data\" ## data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5d84d-468e-49a1-abfb-72ed4299a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCount = storeTokens(audioPath, tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10)\n",
    "fileCount = store_from_librilight(tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10, subset = \"10h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aafd0a-c803-405a-ae0a-59361c9b3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_LENGTH = 30\n",
    "CROP_LENGTH = [15,5,1]\n",
    "\n",
    "semanticDataset = TokensDataset(tokenPath, tokenFile, mode = \"semantic\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)\n",
    "coarseDataset = TokensDataset(tokenPath, tokenFile, mode = \"coarse\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)\n",
    "fineDataset = TokensDataset(tokenPath, tokenFile, mode = \"fine\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec5c99-7810-4a4f-9d78-142585a72e78",
   "metadata": {},
   "source": [
    "## AudioLM: a transformer-based audio model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3753f38-4d04-4cbb-9dfe-19815a11a0f5",
   "metadata": {},
   "source": [
    "### Semantic Transformer: expanding a sentence snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2061771-98ce-44f9-9cc7-3f68a742b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcc5e3-3a3e-4449-98d8-a2b7e8947626",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/semantic-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=15 #30\n",
    "vocab_size=500\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = SemanticTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = SemanticTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96786d9-b25b-499c-93f7-183ea82dc68f",
   "metadata": {},
   "source": [
    "### Coarse Transformer: generating new audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bf990-df9a-4cd2-879b-1e2366fa1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0a13f-7cc6-4d72-9fb3-64932ddd02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/coarse-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=5 #10\n",
    "vocab_size=1024\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = CoarseTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = CoarseTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83b2dd-c6dc-4fe1-b4a9-3b1521921c76",
   "metadata": {},
   "source": [
    "### Fine Transformer: improving audio quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff43e87-43c8-45a0-98c7-bca4615c212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdc491-ae02-438b-a18f-97c149be721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/fine-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=1.5 #3\n",
    "vocab_size=1024\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = FineTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = FineTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e839fc-5c63-4c15-b030-99e079b38380",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e5882-33aa-4899-8210-965c1d11f31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
