{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e39015-58b0-459a-9d16-e558dfbaaf18",
   "metadata": {},
   "source": [
    "# Re-implementation of AudioLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac863704-847f-4414-a525-0ce70fb47f79",
   "metadata": {},
   "source": [
    "In the following notebook, we are going to introduce a reimplementation of the **AudioLM** network, proposed in the paper *\"AudioLM: a Language Modeling Approach to Audio Generation\"* (https://arxiv.org/abs/2209.03143).\n",
    "\n",
    "AudioLM is a state-of-the-art framework built in order to **generate high-quality audio**, while dealing with **long-term consistency**. Trained on a large corpora of audio data, AudioLM is able to provide **natural and coherent audio continuations**, given short initial prompts. The network is also able to **maintain speaker identity**, finding a good trade-off between audio quality and semantical coherence. AudioLM is also able to provide good quality musical continuation from a short prompt, but this will not be discussed in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e88ef7-a623-4482-838e-ff9232d14cb6",
   "metadata": {},
   "source": [
    "## Import and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f59ecc-11cf-424a-ba34-3fa5d6f03ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d4a24-5231-4d13-89e8-2d54176bea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16ebe3-8651-45f1-b6a0-693ab50ae724",
   "metadata": {},
   "source": [
    "## Converting Audio Data into Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021ff75-6391-4116-83b3-81acd618f0db",
   "metadata": {},
   "source": [
    "The most important novelty provided by AudioLM is the usage of a **mixed tokenization approach**, which has never been seen in other Language Modeling competitors. As shown below, we have two tokenization processes that can proceed in parallel.\n",
    "\n",
    "<center><img src=\"reportImg/1.png\"/></center>\n",
    "\n",
    "In order to keep informations regarding language syntax and semantic content in speech, the audio waveform is passed through a **w2v-BERT** model that, combined with a K-Means quantizer, returns a set of **Semantic tokens**.\n",
    "\n",
    "On the other hand, the network needs also to maintain informations about the acoustic features of the audio, in particular pronunciation and speaker identity. In order to do so, the audio waveform is passed through a pretrained audio codec, **SoundStream**, which is able to build an internal hierarchical representation of the audio. Through those representations, called **Acoustic tokens**, the audio is divided into different components, going from the most basic structural audio features (defined as **Coarse Acoustic tokens**) to the fine acoustic details (defined as **Fine Acoustic tokens**).\n",
    "\n",
    "By modeling both semantic and acoustic tokens within the same framework, the semantic tokens would ensure long-term consistency, while the acoustic tokens would ensure high-quality audio synthesis.\n",
    "\n",
    "<center><img src=\"reportImg/2.png\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310502d-4418-4fb4-a335-8302d0a3caa0",
   "metadata": {},
   "source": [
    "### Semantic tokens with w2v-BERT-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25020f93-6094-41b7-9640-87c1f301577a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hubertKM import SemanticTokenizer, visualizeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c836fbe-0044-4409-8627-4ecc024723da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vBERT = SemanticTokenizer(\"facebook/hubert-base-ls960\",\"./hubertKM/hubert_base_ls960_L9_km500.bin\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff722dd-6d36-4e41-8f80-98e04b4a49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f3ff2-9486-45cd-abf5-b83515e5f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce159db-7790-415b-a544-259d6c464d70",
   "metadata": {},
   "source": [
    "### Acoustic tokens with SoundStream codec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19db3a12-9ce3-4add-8d5a-9554d0bba11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoundStream import soundstream_16khz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379563a0-97f8-44a3-9f50-81636b85a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundStream = soundstream_16khz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63a8b6-972b-48ce-8334-a08ac053d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ecfa53-9a2b-4152-8ed2-cdf684567b18",
   "metadata": {},
   "source": [
    "### Token Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2cd0fb-6096-46e7-ae44-78ce0e43f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import storeTokens, TokensDataset, store_from_librilight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0765fa3-fe7f-4ff1-83c4-0e3adba53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenPath = \"out\" ## output file directory\n",
    "tokenFile = \"tokens.csv\" ## output file name\n",
    "audioPath = \"data\" ## data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5d84d-468e-49a1-abfb-72ed4299a306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileCount = storeTokens(audioPath, tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10)\n",
    "fileCount = store_from_librilight(tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10, subset = \"10h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aafd0a-c803-405a-ae0a-59361c9b3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_LENGTH = 30\n",
    "CROP_LENGTH = [15,5,1]\n",
    "\n",
    "semanticDataset = TokensDataset(tokenPath, tokenFile, mode = \"semantic\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)\n",
    "coarseDataset = TokensDataset(tokenPath, tokenFile, mode = \"coarse\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)\n",
    "fineDataset = TokensDataset(tokenPath, tokenFile, mode = \"fine\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec5c99-7810-4a4f-9d78-142585a72e78",
   "metadata": {},
   "source": [
    "## AudioLM: a transformer-based audio model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90c5f82-14a0-4ad2-9a5a-b4d3ecc1e0ca",
   "metadata": {},
   "source": [
    "Once we have converted our data into token sequences, we can start defining the generator model. AudioLM network is based on three Decoder-only transformers, each of them dedicated to the auto-regressive generation of a specific kind of token. \n",
    "During inference, we first generate the new semantic tokens, and then use them to condition the generation of new acoustic tokens. With this structure, we can safely assume that semantic tokens are expected to be conditionally independent from past acoustic tokens given past semantic tokens:\n",
    "$$\n",
    "  p(z_{t}|z_{\\lt t},y_{\\lt t}) \\simeq p(z_{t}|z_{\\lt t})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3753f38-4d04-4cbb-9dfe-19815a11a0f5",
   "metadata": {},
   "source": [
    "### Semantic Transformer: expanding a sentence snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b14089-52c2-4296-a030-83f3fa2775fd",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla rhoncus elementum neque nec suscipit. Cras hendrerit feugiat dolor at sodales. Proin feugiat mattis felis vel maximus. Quisque tempus imperdiet odio, eget rhoncus eros tempus nec. Fusce venenatis est et dui porta fermentum a nec mauris. Aenean sit amet ullamcorper est. Pellentesque semper lorem fermentum vulputate egestas. Vestibulum interdum viverra felis. Maecenas molestie pulvinar consectetur. Curabitur vitae dignissim massa. Sed sodales odio ante, ut mollis sem feugiat ullamcorper. In hac habitasse platea dictumst. Aliquam et ante dui. Fusce laoreet orci in orci tincidunt, vitae mollis mi vestibulum. Nulla aliquam volutpat purus, suscipit iaculis metus egestas vel. Suspendisse pretium bibendum turpis ac dictum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4c8f9-cf02-45a5-956b-ebae40f7d5e1",
   "metadata": {},
   "source": [
    "<center><img src=\"reportImg/semantic.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2061771-98ce-44f9-9cc7-3f68a742b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcc5e3-3a3e-4449-98d8-a2b7e8947626",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/semantic-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=15 #30\n",
    "vocab_size=500\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = SemanticTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = SemanticTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96786d9-b25b-499c-93f7-183ea82dc68f",
   "metadata": {},
   "source": [
    "### Coarse Transformer: generating new audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7266b-c51a-4a2a-80d1-3fedd1b7493b",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla rhoncus elementum neque nec suscipit. Cras hendrerit feugiat dolor at sodales. Proin feugiat mattis felis vel maximus. Quisque tempus imperdiet odio, eget rhoncus eros tempus nec. Fusce venenatis est et dui porta fermentum a nec mauris. Aenean sit amet ullamcorper est. Pellentesque semper lorem fermentum vulputate egestas. Vestibulum interdum viverra felis. Maecenas molestie pulvinar consectetur. Curabitur vitae dignissim massa. Sed sodales odio ante, ut mollis sem feugiat ullamcorper. In hac habitasse platea dictumst. Aliquam et ante dui. Fusce laoreet orci in orci tincidunt, vitae mollis mi vestibulum. Nulla aliquam volutpat purus, suscipit iaculis metus egestas vel. Suspendisse pretium bibendum turpis ac dictum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed3993-72ab-4f88-8a94-5e9592e79e0a",
   "metadata": {},
   "source": [
    "<center><img src=\"reportImg/coarse.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bf990-df9a-4cd2-879b-1e2366fa1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0a13f-7cc6-4d72-9fb3-64932ddd02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/coarse-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=5 #10\n",
    "vocab_size=1024\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = CoarseTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = CoarseTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83b2dd-c6dc-4fe1-b4a9-3b1521921c76",
   "metadata": {},
   "source": [
    "### Fine Transformer: generating audio details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f651d-2e85-4c76-9a01-d68439c5887d",
   "metadata": {},
   "source": [
    "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla rhoncus elementum neque nec suscipit. Cras hendrerit feugiat dolor at sodales. Proin feugiat mattis felis vel maximus. Quisque tempus imperdiet odio, eget rhoncus eros tempus nec. Fusce venenatis est et dui porta fermentum a nec mauris. Aenean sit amet ullamcorper est. Pellentesque semper lorem fermentum vulputate egestas. Vestibulum interdum viverra felis. Maecenas molestie pulvinar consectetur. Curabitur vitae dignissim massa. Sed sodales odio ante, ut mollis sem feugiat ullamcorper. In hac habitasse platea dictumst. Aliquam et ante dui. Fusce laoreet orci in orci tincidunt, vitae mollis mi vestibulum. Nulla aliquam volutpat purus, suscipit iaculis metus egestas vel. Suspendisse pretium bibendum turpis ac dictum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d4d86-bb63-43dc-b456-04493ea4d9d8",
   "metadata": {},
   "source": [
    "<center><img src=\"reportImg/fine.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff43e87-43c8-45a0-98c7-bca4615c212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdc491-ae02-438b-a18f-97c149be721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/fine-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=1.5 #3\n",
    "vocab_size=1024\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = FineTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = FineTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e839fc-5c63-4c15-b030-99e079b38380",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e5882-33aa-4899-8210-965c1d11f31f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
