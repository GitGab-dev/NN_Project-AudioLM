{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e39015-58b0-459a-9d16-e558dfbaaf18",
   "metadata": {},
   "source": [
    "# Re-implementation of AudioLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac863704-847f-4414-a525-0ce70fb47f79",
   "metadata": {},
   "source": [
    "In the following notebook, we are going to introduce a reimplementation of the **AudioLM** network, proposed in the paper *\"AudioLM: a Language Modeling Approach to Audio Generation\"* (https://arxiv.org/abs/2209.03143).\n",
    "\n",
    "AudioLM is a state-of-the-art framework built in order to **generate high-quality audio**, while dealing with **long-term consistency**. Trained on a large corpora of audio data, AudioLM is able to provide **natural and coherent audio continuations**, given short initial prompts. The network is also able to **maintain speaker identity**, finding a good trade-off between audio quality and semantical coherence. AudioLM is also able to provide good quality musical continuation from a short prompt, but this will not be discussed in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e88ef7-a623-4482-838e-ff9232d14cb6",
   "metadata": {},
   "source": [
    "## Import and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b7982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for Colab\n",
    "\n",
    "try:\n",
    "    import scripts.hubertKM\n",
    "    print(\"Working on local\")\n",
    "except ImportError:\n",
    "    from google.colab import drive\n",
    "    import os\n",
    "    import sys\n",
    "    \n",
    "    print(\"Working on colab\")\n",
    "\n",
    "    drive.mount('/content/drive') \n",
    "\n",
    "    audio_lm_path = '/content/drive/My Drive/AudioLM'\n",
    "\n",
    "    # Create the folder if it does not exist\n",
    "    if not os.path.exists(audio_lm_path):\n",
    "        os.makedirs(audio_lm_path)\n",
    "        print(f\"Folder created at: {audio_lm_path}\")\n",
    "    else:\n",
    "        print(f\"Folder already exists at: {audio_lm_path}\")\n",
    "\n",
    "    sys.path.append(audio_lm_path)\n",
    "\n",
    "    %cd {audio_lm_path}\n",
    "    !git clone https://github.com/GitGab-dev/NN_Project-AudioLM\n",
    "    %cd NN_Project-AudioLM\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f59ecc-11cf-424a-ba34-3fa5d6f03ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic import\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# data imports\n",
    "from scripts.hubertKM import SemanticTokenizer\n",
    "from scripts.SoundStream import soundstream_16khz, audio_to_tokens, tokens_to_audio\n",
    "from IPython.display import Audio, display\n",
    "from scripts.data import store_from_librilight, getSingleModelDataLoaders, prepare_single_audio\n",
    "\n",
    "# model imports\n",
    "from scripts.TransformerModel import createSingleModel, generate_new_sequence\n",
    "\n",
    "# trainer imports\n",
    "from scripts.modelTrainer import setSingleTrainer, modelFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba2d4a24-5231-4d13-89e8-2d54176bea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16ebe3-8651-45f1-b6a0-693ab50ae724",
   "metadata": {},
   "source": [
    "## Converting Audio Data into Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021ff75-6391-4116-83b3-81acd618f0db",
   "metadata": {},
   "source": [
    "The most important novelty provided by AudioLM is the usage of a **mixed tokenization approach**, which has never been seen in other Language Modeling competitors. As shown below, we have two tokenization processes that can proceed in parallel.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/1.png\"/></center>\n",
    "\n",
    "In order to keep informations regarding language syntax and semantic content in speech, the audio waveform is passed through a **w2v-BERT** model that, combined with a K-Means quantizer, returns a set of **Semantic tokens**.\n",
    "\n",
    "On the other hand, the network needs also to maintain informations about the acoustic features of the audio, in particular pronunciation and speaker identity. In order to do so, the audio waveform is passed through a pretrained audio codec, **SoundStream**, which is able to build an internal hierarchical representation of the audio. Through those representations, called **Acoustic tokens**, the audio is divided into different components, going from the most basic structural audio features (defined as **Coarse Acoustic tokens**) to the fine acoustic details (defined as **Fine Acoustic tokens**).\n",
    "\n",
    "By modeling both semantic and acoustic tokens within the same framework, the semantic tokens would ensure long-term consistency, while the acoustic tokens would ensure high-quality audio synthesis.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/2.png\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310502d-4418-4fb4-a335-8302d0a3caa0",
   "metadata": {},
   "source": [
    "### Semantic tokens with w2v-BERT-like model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00fe9fa-2535-472c-8ed6-173f5f9690e8",
   "metadata": {},
   "source": [
    "In the original paper, the semantic tokens are computed through **w2v-BERT**, a recent model trained on learning **self-supervised audio representations**. When trained on large speech corpora, w2v-BERT learns to map the input audio waveform to a rich set of linguistic features.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/w2vbert.png\"/></center>\n",
    "\n",
    "The model is based on the usage of **Conformer Blocks**, a variation of Transformer blocks augmented with convolution, and during training uses a combination of two self-supervised objectives: a **masked language modeling(MLM) loss** and a **contrastive loss**. Even though this model can be fine-tuned for different tasks, **AudioLM leverages only on the pre-trained version**, using the Context Vectors of as specific MLM Layer as embeddings.\n",
    "Finally the embeddings are passed through a **K-Means quantizer**, that would simply convert the embeddings into centroid indices, which will be used as Semantic Tokens. During training, the paper propose a value of K=1024 clusters for the quantizer, choosing as Context Vectors the output of the 7th MLM layer normalized.\n",
    "\n",
    "Regarding our implementation, since **w2v-BERT is a closed-source project**, we opted for a **HuBERT** model as an alternative. We decided to work with a **pretrained version of HuBERT-Base and K-Means** quantizer provided by Fairseq, at https://github.com/facebookresearch/fairseq/blob/main/examples/hubert. In the following model, we use K=500 clusters and the output is taken from the 9th layer of HuBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c836fbe-0044-4409-8627-4ecc024723da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vBERT = SemanticTokenizer(\"facebook/hubert-base-ls960\",\"./KmWeights/hubert_base_ls960_L9_km500.bin\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff722dd-6d36-4e41-8f80-98e04b4a49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_audio = \"exampleAudio.flac\"\n",
    "waveform, sampleRate = torchaudio.load(example_audio)\n",
    "\n",
    "with torch.no_grad():\n",
    "    semanticTokens, embeddings = w2vBERT(waveform)\n",
    "\n",
    "print(\"Embeddings taken from HuBERT\\n\")\n",
    "print(embeddings)\n",
    "print(\"\\nSemantic Tokens\\n\")\n",
    "print(semanticTokens[:30],\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce159db-7790-415b-a544-259d6c464d70",
   "metadata": {},
   "source": [
    "### Acoustic tokens with SoundStream codec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3dfad4-2744-4071-96af-dfb8d1ef5058",
   "metadata": {},
   "source": [
    "The original framework computes the acoustic tokens using **SoundStream**, a state-of-the-art **neural audio codec** that, through the usage of a convolutional enconder, is able to **map the input audio into a new lower-sampled version**. This allows the model to maintain information about acoustic features, while keeping the audio quality as much as unaffected as possible.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/soundstream.png\"/></center>\n",
    "\n",
    "The model follows a **common Encoder-Decoder structure**, while the middle part, a **Residual Vector Quantizer**, is responsible to catch all the **meaningful acoustic aspects** of the input waveform, classifing them into a hierarchical order. The codec achieves high quality by being trained end-to-end with a combination of **reconstruction and adversarial losses**.\n",
    "\n",
    "For AudioLM, the **acoustic tokens are the output embeddings** obtained by the RVQ component, in quantizer order. As already mentioned, the first quantizers capture elementary acoustics, while the last quantizers will encapsulates more fine audio details. In the paper, they proposed using 12 quantizer layers (where the first $Q' = 4$ are used for Coarse tokens), along with a codebook of 1024 and 4 encoder/decoder blocks.\n",
    "\n",
    "In our implementation, we used a **pretrained SoundStream model** implementation to reduce training time, provided by https://github.com/kaiidams/soundstream-pytorch. In this model version, only **the number of quantizers is reduced** to 8, where 3 of them are dedicated to Coarse tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379563a0-97f8-44a3-9f50-81636b85a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundStream = soundstream_16khz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63a8b6-972b-48ce-8334-a08ac053d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_audio = \"exampleAudio.flac\"\n",
    "waveform, sampleRate = torchaudio.load(example_audio)\n",
    "\n",
    "coarse, fine = audio_to_tokens(waveform, soundStream)\n",
    "\n",
    "print(\"Coarse Tokens:\")\n",
    "print(coarse)\n",
    "\n",
    "print(\"\\nFine Tokens:\")\n",
    "print(fine)\n",
    "\n",
    "reconstructedWaveform = tokens_to_audio(coarse, fine, soundStream)\n",
    "\n",
    "print(\"\\nOriginal Audio:\\n\")\n",
    "display(Audio(waveform, rate = sampleRate))\n",
    "\n",
    "print(\"\\nReconstructed Audio:\\n\")\n",
    "display(Audio(reconstructedWaveform, rate = sampleRate))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ecfa53-9a2b-4152-8ed2-cdf684567b18",
   "metadata": {},
   "source": [
    "### Token Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c666fc8-e242-4a07-b22c-907cbcbc0dcf",
   "metadata": {},
   "source": [
    "Now that we have defined models for both the Semantic and Acoustic tokenization, we can proceed with **converting audio data into tokens**. We tried **performing this operation separately** from the AudioLM training, in order to allievate computational costs.\n",
    "\n",
    "The data used for the model comes from **Libri-Light** (at https://github.com/facebookresearch/libri-light), an high-quality collection of audio-books narrated by different speakers. The dataset already comes with a nice separation in training data and finetuning data. \n",
    "\n",
    "In the original paper, they used *unlab-60k*, a combined version of small, medium and big dataset, for a total of about **60000 hours** worth of recording. Since we do not have any computational mean allowing us to train a network on such amount of data, we tried using the **fine-tuning version**, that is worth about **10 hours** of audio data.\n",
    "\n",
    "Our implementation takes in input the data from **Libri-Light Limited Torch dataset** (refer to https://pytorch.org/audio/main/generated/torchaudio.datasets.LibriLightLimited.html), and return a **CSV file** containing the Semantic, Coarse and Fine tokens. \n",
    "\n",
    "During dataset creation, we crop our training data in pieces, as suggested by the paper, but we used **halved durations** (segments of 15, 5, 1 seconds respectively for Semantic, Coarse and Fine transformers). We also provided two version of dataset creation: *storeTokens(...)* allows to use local audio data, while  *store_from_librilight(...)* uses the before mentioned Torch dataset. We used the second version for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0765fa3-fe7f-4ff1-83c4-0e3adba53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenPath = \"out\" ## output file directory\n",
    "tokenFile = \"tokens.csv\" ## output file name\n",
    "audioPath = \"data\" ## data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa5d84d-468e-49a1-abfb-72ed4299a306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fileCount = storeTokens(audioPath, tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10)\n",
    "fileCount = store_from_librilight(tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10, subset = \"10h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1aafd0a-c803-405a-ae0a-59361c9b3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_LENGTH = 30\n",
    "CROP_LENGTH = [15,5,1] #Original is [30, 10, 3]\n",
    "USE_OFFSET = False\n",
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "MAX_EPOCH = 30\n",
    "\n",
    "common_params = {\n",
    "\t'tokenPath' : tokenPath, \n",
    "\t'tokenFile' : tokenFile, \n",
    "\t'expected_audio_length' : 30, \n",
    "\t'crop_length' : CROP_LENGTH, \n",
    "\t'useOffset' : USE_OFFSET, \n",
    "\t'training_percentage' : TRAINING_PERCENTAGE, \n",
    "\t'batch_size' : BATCH_SIZE\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec5c99-7810-4a4f-9d78-142585a72e78",
   "metadata": {},
   "source": [
    "## AudioLM: a transformer-based audio model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256a972-8bf8-4d56-a58a-d1b76d5bfda1",
   "metadata": {},
   "source": [
    "Once we have converted our data into token sequences, we can start defining the **generator** model. AudioLM network is based on three Decoder-only transformers, each of them dedicated to the **auto-regressive** generation of a specific kind of token. \n",
    "During inference, we first generate the new semantic tokens, and then use them to condition the generation of new acoustic tokens. With this structure, we can safely assume that *semantic tokens are expected to be conditionally independent from past acoustic tokens* given past semantic tokens:\n",
    "$$\n",
    "  p(z_{t}|z_{\\lt t},y_{\\lt t}) \\simeq p(z_{t}|z_{\\lt t})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4537dcd-f043-470d-be09-bb24357fe517",
   "metadata": {},
   "source": [
    "### Some implementation details\n",
    "\n",
    "In the original paper, each generator follows **the same identical decoder-only Transformer** structure, with:\n",
    "- 12 decoder layers\n",
    "- 16 attention heads\n",
    "- 1024 as embedding dimension\n",
    "- 4096 as dimension for the feed-forward layer\n",
    "- 0.1 as dropout value\n",
    "\n",
    "The original model is also enriched with a **T5-style relative positional encoding**, that is trained along with the three stage transformers. For this reimplementation, we adapted an implementation of the paper *\"Self-Attention with Relative Position Representations\"* provided by https://github.com/AliHaiderAhmad001/Self-Attention-with-Relative-Position-Representations. We also reduced the size of the model for computational reasons, as it will be shown later. In general, we tried to reconstruct the original structure of the Decoders, considering that the **original model is closed-source**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ba1f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "\t'd_model': 256, #1024\n",
    "\t'num_layers': 3, #12\n",
    "\t'num_heads': 4, #16\n",
    "\t'dim_feedforward': 1024, #4096\n",
    "\t'dropout': 0.1,\n",
    "\t'learning_rate': 10e-4,\n",
    "\t'myDevice': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "\t'Q_prime' : 3,\n",
    "\t'Q' : 8\n",
    "}\n",
    "\n",
    "#N.B. The semantic vocab_size should always be 500, the other two should be 1024 * Q_prime and 1024 * Q respectively if we use the offset, 1024 if we don't\n",
    "if USE_OFFSET:\n",
    "\tVOCAB_SIZE = [500, 1024 * model_params['Q_prime'], 1024 * model_params['Q']]\n",
    "else:\n",
    "\tVOCAB_SIZE = [500, 1024, 1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3753f38-4d04-4cbb-9dfe-19815a11a0f5",
   "metadata": {},
   "source": [
    "### Semantic Transformer: expanding a sentence snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4c8f9-cf02-45a5-956b-ebae40f7d5e1",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/semantic.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b14089-52c2-4296-a030-83f3fa2775fd",
   "metadata": {},
   "source": [
    "The first transformer is responsible for the generation of new semantic tokens, estimating $ p(z_{t}|z_{\\lt t}) $, where $ z_{i} $ denotes the $i$-th semantic token. In this way, given the semantic aspects of the prompt, the network is able to generate **new coherent content**, keeping long-term consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2061771-98ce-44f9-9cc7-3f68a742b1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_train_dataloader, semantic_valid_dataloader = getSingleModelDataLoaders(mode = \"semantic\", **common_params)\n",
    "\n",
    "semantic_trainer = setSingleTrainer(max_epoch = MAX_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcc5e3-3a3e-4449-98d8-a2b7e8947626",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_checkpoint = \"checkpoints/semantic-checkpoint-10h-colab.ckpt\"\n",
    "\n",
    "semantic_model = createSingleModel(type = \"semantic\", checkpoint_path = semantic_checkpoint, **model_params, audioDuration = CROP_LENGTH[0], vocab_size = VOCAB_SIZE[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d09c68-8cd0-46e2-9439-78899c395d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFit(model = semantic_model, trainer = semantic_trainer, train_loader = semantic_train_dataloader, valid_loader = semantic_valid_dataloader, checkpoint_path = semantic_checkpoint, myDevice=model_params['myDevice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96786d9-b25b-499c-93f7-183ea82dc68f",
   "metadata": {},
   "source": [
    "### Coarse Transformer: generating new audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed3993-72ab-4f88-8a94-5e9592e79e0a",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/coarse.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7266b-c51a-4a2a-80d1-3fedd1b7493b",
   "metadata": {},
   "source": [
    "Once we have defined a mechanism to generate new semantic content, we need to generate the corresponding acoustic content. Since we want the new tokens to be **coherent both with the sentence meaning** (represented by the Semantic tokens) and the **original speaker and enviroment conditions** (represented by the past Coarse Tokens), this model is trained to estimate:\n",
    "$$ p(y_t^q \\mid z, y_{<t}^{<Q'}, y_t^{<q})  \\quad \\text{for} \\ q \\le Q'$$\n",
    "where the current generated semantic token depends on **Semantic tokens** $z$ and the **acoustic tokens** $ y_{<t}^{<Q'} $ and $ y_t^{<q} $ that references respectively to **previous and current audio parts**. In the formula, $Q'$ denotes the **number of Soundstream quantizer** dedicated to Coarse tokens generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bf990-df9a-4cd2-879b-1e2366fa1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_train_dataloader, coarse_valid_dataloader = getSingleModelDataLoaders(mode = \"coarse\", **common_params)\n",
    "\n",
    "coarse_trainer = setSingleTrainer(max_epoch = MAX_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0a13f-7cc6-4d72-9fb3-64932ddd02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarse_checkpoint = \"checkpoints/coarse-checkpoint-10h-colab.ckpt\"\n",
    "\n",
    "coarse_model = createSingleModel(type = \"coarse\", checkpoint_path = coarse_checkpoint, **model_params, audioDuration = CROP_LENGTH[1], vocab_size=VOCAB_SIZE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac8d88-f580-4454-a59d-dfb4942e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFit(model = coarse_model, trainer = coarse_trainer, train_loader = coarse_train_dataloader, valid_loader = coarse_valid_dataloader, checkpoint_path = coarse_checkpoint, myDevice=model_params['myDevice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83b2dd-c6dc-4fe1-b4a9-3b1521921c76",
   "metadata": {},
   "source": [
    "### Fine Transformer: rebuilding a detailed audio waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d4d86-bb63-43dc-b456-04493ea4d9d8",
   "metadata": {},
   "source": [
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/fine.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f651d-2e85-4c76-9a01-d68439c5887d",
   "metadata": {},
   "source": [
    "At last, we need to generate the **acoustic details** of the extended audio, since we need to recreate the **hierarchical structure** of acoustic features needed for the SoundStream model. This is easily done through a third and last transformer, that given the Coarse tokens and the previous Fine tokens, is trained to generate new Fine tokens. In formula:\n",
    "\n",
    "$$ p(y_t^q \\mid y_{<t}^{\\leq Q'}, y_{\\geq t}^{> Q'}, y_t^{<q}) \\quad \\text{for} \\ q > Q' $$\n",
    "\n",
    "where the new tokens depends on the **Coarse tokens** $y_{<t}^{\\leq Q'}$ and the **Fine tokens** $y_{\\geq t}^{> Q'}$ and $y_t^{<q}$, belonging respectively to **previous and current audio parts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff43e87-43c8-45a0-98c7-bca4615c212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_train_dataloader, fine_valid_dataloader = getSingleModelDataLoaders(mode = \"fine\", **common_params)\n",
    "fine_trainer = setSingleTrainer(max_epoch = MAX_EPOCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdc491-ae02-438b-a18f-97c149be721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_checkpoint = \"checkpoints/fine-checkpoint-10h-colab.ckpt\"\n",
    "\n",
    "fine_model = createSingleModel(type = \"fine\", checkpoint_path = fine_checkpoint, **model_params, audioDuration = CROP_LENGTH[2], vocab_size = VOCAB_SIZE[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fd886-e05f-4da2-bcea-75b57a614262",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelFit(model = fine_model, trainer = fine_trainer, train_loader = fine_train_dataloader, valid_loader = fine_valid_dataloader, checkpoint_path = fine_checkpoint, myDevice=model_params['myDevice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e839fc-5c63-4c15-b030-99e079b38380",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907d0777",
   "metadata": {},
   "source": [
    "### How to extend an audio input\n",
    "\n",
    "After the training we can try use our models. First of all we prepare a single audio input using Hubert and SoundStream and then, through the AudioLM model, we can **generate a new sequence** with our preferred length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "311e5882-33aa-4899-8210-965c1d11f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_tokens, coarse_tokens, fine_tokens = prepare_single_audio(\"exampleAudio.flac\", w2vBERT, soundStream, audioDuration=2, useOffset = USE_OFFSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc3c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_acoustic_tokens = generate_new_sequence(semantic_tokens, coarse_tokens, fine_tokens, semantic_model, coarse_model, fine_model, audioDuration=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc15ab-c234-4aa0-b039-bf528c361d03",
   "metadata": {},
   "source": [
    "Once we have generated both Coarse acoustic tokens and Fine acoustic tokens, we can **reshape them into a format that resembles the original data** provided by the Residual Vector Quantizer component of SoundStream. Finally we pass the Acoustic tokens into the Decoder part of Soundstream, that will return the **final audio waveform**.\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/full.png\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d4a78a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform = tokens_to_audio(coarse_tokens=generated_acoustic_tokens[0], fine_tokens=generated_acoustic_tokens[1], model=soundStream, removeOffsets=USE_OFFSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The extended audio:\\n\")\n",
    "display(Audio(waveform, rate = 16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202b5aa-45b9-466b-ab4e-88e0630d06c0",
   "metadata": {},
   "source": [
    "### Training results\n",
    "\n",
    "Because of **prohibiting computational time and lack of resources**, we tried to move the training from local CPU to Colab. We noticed that, even though the **training time has decreased by a lot**, the memory limit imposed by **Colab didn't allow to load the entire model**.\n",
    "\n",
    "We opted for reducing the structure of the model, changing the number of layers and parameters. We worked on two version of the model: the first one uses about a **quarter** of the original parameters values, while the second one uses **halfed** values for the parameters. Using the full model caused a **\"Out of Memory error\"** in Colab.\n",
    "\n",
    "For the same reason, **the training of the model over 60000 hours of data, as suggested by the paper, was not feasible**, both for time and memory constraints given by Colab service. So we decided to make some tests on both 1 hour and 10 hours of data, and analyse the difference between the two approaches.\n",
    "\n",
    "For the same reasons, only the Coarse Transformer has been trained. In fact, while training three transformers has been proved unfeasible, we considered the Coarse Transformer as the best candidate to train because:\n",
    "\n",
    "- With respect to the Semantic Transformer, it uses a condition input sequence;\n",
    "- With respect to the Fine Transformer, the size of the vocabulary and the input sequence length were more manageable.\n",
    "- Inference on the Coarse Transformer gives the more interpretable results, giving a coarse but comprehensible audio result.\n",
    "\n",
    "Every test shown below is the result obtained by 30 epochs of training.\n",
    "\n",
    "Here we show some tests on 1 hour of data. We tried **three different cases**:\n",
    "\n",
    "- A model that uses both the offsets for Acoustic Tokens (increasing the vocab_size parameter) and a dual embedding for conditioning and generator tokens;\n",
    "- A model that uses only the offsets, without dual embedding\n",
    "- A model that uses neither of them\n",
    "\n",
    "As seen below, the reduced Quarter-model responds well on the most basic alternative, while the Halfed-model appears to get some benefits from using offsets on acoustic tokens. Dual embedding usage doesn't seem to provide any advantage, at least while tested in these two conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b01305-bd95-4145-afcf-601a05a4b9ef",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/Q-1a.svg\"/></th>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/H-1a.svg\"/></th>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/Q-1b.svg\"/></th>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/H-1b.svg\"/></th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/Q-1c.svg\"/></th>\n",
    "        <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/H-1c.svg\"/></th>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4aebd8-9456-4eb6-a47f-8bf3226a4fb3",
   "metadata": {},
   "source": [
    "At last, we report the results obtained with **10 hours of data**. As we can see, both the **accuracy and validation loss have improved** with the increased amount of used data. This suggests that, since the model uses a Transformer based architecture, the **amount of data affects significantly the generator performance**. It's also interesting to observe that changing the number of parameters doesn't seem to particularly affect the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875725a8-f8b1-4fbf-9da4-ffb0177d63df",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/Q-10a.svg\"/></th>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/H-10a.svg\"/></th>\n",
    "    \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/Q-10b.svg\"/></th>\n",
    "    <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/H-10b.svg\"/></th>\n",
    "  </tr>\n",
    "    <tr>\n",
    "        <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/Q-10c.svg\"/></th>\n",
    "        <th><img src=\"https://raw.githubusercontent.com/GitGab-dev/NN_Project-AudioLM/main/reportImg/H-10c.svg\"/></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b202049-fa9d-4f90-85bf-3897218d6ff0",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c2618-cf9a-49a2-9e72-c351a6fbe3b9",
   "metadata": {},
   "source": [
    "- AudioLM paper: https://arxiv.org/abs/2209.03143\n",
    "- AudioLM blog post: https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\n",
    "- HuBERT and K-Means implementation: https://github.com/facebookresearch/fairseq/blob/main/examples/hubert\n",
    "- w2v-BERT paper: https://arxiv.org/abs/2108.06209\n",
    "- SoundStream implementation: https://github.com/kaiidams/soundstream-pytorch\n",
    "- SoundStream paper: https://arxiv.org/abs/2107.03312\n",
    "- LibriLight full dataset: https://github.com/facebookresearch/libri-light/tree/main\n",
    "- LibriLight Limited on pyTorch: https://pytorch.org/audio/main/generated/torchaudio.datasets.LibriLightLimited.html\n",
    "- Relative positional embeddings implementation: https://github.com/AliHaiderAhmad001/Self-Attention-with-Relative-Position-Representations\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
