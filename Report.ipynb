{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e39015-58b0-459a-9d16-e558dfbaaf18",
   "metadata": {},
   "source": [
    "# Re-implementation of AudioLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac863704-847f-4414-a525-0ce70fb47f79",
   "metadata": {},
   "source": [
    "In the following notebook, we are going to introduce a reimplementation of the **AudioLM** network, proposed in the paper *\"AudioLM: a Language Modeling Approach to Audio Generation\"* (https://arxiv.org/abs/2209.03143).\n",
    "\n",
    "AudioLM is a state-of-the-art framework built in order to **generate high-quality audio**, while dealing with **long-term consistency**. Trained on a large corpora of audio data, AudioLM is able to provide **natural and coherent audio continuations**, given short initial prompts. The network is also able to **maintain speaker identity**, finding a good trade-off between audio quality and semantical coherence. AudioLM is also able to provide good quality musical continuation from a short prompt, but this will not be discussed in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e88ef7-a623-4482-838e-ff9232d14cb6",
   "metadata": {},
   "source": [
    "## Import and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07f59ecc-11cf-424a-ba34-3fa5d6f03ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic import\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "# data imports\n",
    "from hubertKM import SemanticTokenizer\n",
    "from SoundStream import soundstream_16khz\n",
    "from data import TokensDataset, store_from_librilight\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# model imports\n",
    "from TransformerModel import SemanticTransformer, CoarseTransformer, FineTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2d4a24-5231-4d13-89e8-2d54176bea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)  # If using CUDA\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16ebe3-8651-45f1-b6a0-693ab50ae724",
   "metadata": {},
   "source": [
    "## Converting Audio Data into Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6021ff75-6391-4116-83b3-81acd618f0db",
   "metadata": {},
   "source": [
    "The most important novelty provided by AudioLM is the usage of a **mixed tokenization approach**, which has never been seen in other Language Modeling competitors. As shown below, we have two tokenization processes that can proceed in parallel.\n",
    "\n",
    "<center><img src=\"reportImg/1.png\"/></center>\n",
    "\n",
    "In order to keep informations regarding language syntax and semantic content in speech, the audio waveform is passed through a **w2v-BERT** model that, combined with a K-Means quantizer, returns a set of **Semantic tokens**.\n",
    "\n",
    "On the other hand, the network needs also to maintain informations about the acoustic features of the audio, in particular pronunciation and speaker identity. In order to do so, the audio waveform is passed through a pretrained audio codec, **SoundStream**, which is able to build an internal hierarchical representation of the audio. Through those representations, called **Acoustic tokens**, the audio is divided into different components, going from the most basic structural audio features (defined as **Coarse Acoustic tokens**) to the fine acoustic details (defined as **Fine Acoustic tokens**).\n",
    "\n",
    "By modeling both semantic and acoustic tokens within the same framework, the semantic tokens would ensure long-term consistency, while the acoustic tokens would ensure high-quality audio synthesis.\n",
    "\n",
    "<center><img src=\"reportImg/2.png\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f310502d-4418-4fb4-a335-8302d0a3caa0",
   "metadata": {},
   "source": [
    "### Semantic tokens with w2v-BERT-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c836fbe-0044-4409-8627-4ecc024723da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gabriele\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "w2vBERT = SemanticTokenizer(\"facebook/hubert-base-ls960\",\"./hubertKM/hubert_base_ls960_L9_km500.bin\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff722dd-6d36-4e41-8f80-98e04b4a49ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f3ff2-9486-45cd-abf5-b83515e5f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce159db-7790-415b-a544-259d6c464d70",
   "metadata": {},
   "source": [
    "### Acoustic tokens with SoundStream codec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379563a0-97f8-44a3-9f50-81636b85a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundStream = soundstream_16khz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d63a8b6-972b-48ce-8334-a08ac053d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## missing model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ecfa53-9a2b-4152-8ed2-cdf684567b18",
   "metadata": {},
   "source": [
    "### Token Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0765fa3-fe7f-4ff1-83c4-0e3adba53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenPath = \"out\" ## output file directory\n",
    "tokenFile = \"tokens.csv\" ## output file name\n",
    "audioPath = \"data\" ## data_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fa5d84d-468e-49a1-abfb-72ed4299a306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 570M/570M [00:38<00:00, 15.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVED 10 AUDIO ON OUTPUT out\\tokens.csv. Total of 10 records saved.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#fileCount = storeTokens(audioPath, tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m fileCount \u001b[38;5;241m=\u001b[39m \u001b[43mstore_from_librilight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenFile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2vBERT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoundStream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfileCountCheckpoint\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10h\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\data.py:354\u001b[0m, in \u001b[0;36mstore_from_librilight\u001b[1;34m(outDir, outFile, w2vBERT, soundStream, fileCountCheckpoint, subset, lenght)\u001b[0m\n\u001b[0;32m    352\u001b[0m waveform, sr \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mresample(waveform, sr, \u001b[38;5;241m16000\u001b[39m), \u001b[38;5;241m16000\u001b[39m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 354\u001b[0m     semanticTokens, _ \u001b[38;5;241m=\u001b[39m \u001b[43mw2vBERT\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     coarseTokens, fineTokens \u001b[38;5;241m=\u001b[39m audio_to_tokens(waveform, soundStream)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lenght \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\hubertKM.py:24\u001b[0m, in \u001b[0;36mSemanticTokenizer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 24\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhidden_states[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchosenOutputLevel]\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Normalize embeddings (not possible with pretrained models)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m#mean = embeddings.mean(dim=1, keepdim=True)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m#std = embeddings.std(dim=1, keepdim=True)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m#normalizedEmbeddings = (embeddings - mean) / std\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     normalizedEmbeddings \u001b[38;5;241m=\u001b[39m embeddings\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:1340\u001b[0m, in \u001b[0;36mHubertModel.forward\u001b[1;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1335\u001b[0m output_hidden_states \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1336\u001b[0m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39moutput_hidden_states\n\u001b[0;32m   1337\u001b[0m )\n\u001b[0;32m   1338\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1340\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1341\u001b[0m extract_features \u001b[38;5;241m=\u001b[39m extract_features\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   1343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1344\u001b[0m     \u001b[38;5;66;03m# compute reduced attention_mask corresponding to feature vectors\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:360\u001b[0m, in \u001b[0;36mHubertFeatureEncoder.forward\u001b[1;34m(self, input_values)\u001b[0m\n\u001b[0;32m    355\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    356\u001b[0m             conv_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    357\u001b[0m             hidden_states,\n\u001b[0;32m    358\u001b[0m         )\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 360\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mconv_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\transformers\\models\\hubert\\modeling_hubert.py:204\u001b[0m, in \u001b[0;36mHubertNoLayerNormConvLayer.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m--> 204\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(hidden_states)\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:308\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\NN_Project-AudioLM_2\\env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:304\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    302\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    303\u001b[0m                     _single(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#fileCount = storeTokens(audioPath, tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10)\n",
    "fileCount = store_from_librilight(tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10, subset = \"10h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1aafd0a-c803-405a-ae0a-59361c9b3097",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_LENGTH = 30\n",
    "CROP_LENGTH = [15,5,1]\n",
    "\n",
    "semanticDataset = TokensDataset(tokenPath, tokenFile, mode = \"semantic\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)\n",
    "coarseDataset = TokensDataset(tokenPath, tokenFile, mode = \"coarse\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)\n",
    "fineDataset = TokensDataset(tokenPath, tokenFile, mode = \"fine\", expected_audio_length = AUDIO_LENGTH, crop_length = CROP_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ec5c99-7810-4a4f-9d78-142585a72e78",
   "metadata": {},
   "source": [
    "## AudioLM: a transformer-based audio model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d256a972-8bf8-4d56-a58a-d1b76d5bfda1",
   "metadata": {},
   "source": [
    "Once we have converted our data into token sequences, we can start defining the **generator** model. AudioLM network is based on three Decoder-only transformers, each of them dedicated to the **auto-regressive** generation of a specific kind of token. \n",
    "During inference, we first generate the new semantic tokens, and then use them to condition the generation of new acoustic tokens. With this structure, we can safely assume that *semantic tokens are expected to be conditionally independent from past acoustic tokens* given past semantic tokens:\n",
    "$$\n",
    "  p(z_{t}|z_{\\lt t},y_{\\lt t}) \\simeq p(z_{t}|z_{\\lt t})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4537dcd-f043-470d-be09-bb24357fe517",
   "metadata": {},
   "source": [
    "### Some implementation details\n",
    "\n",
    "In the original paper, each generator follows **the same identical decoder-only Transformer** structure, with:\n",
    "- 12 decoder layers\n",
    "- 16 attention heads\n",
    "- 1024 as embedding dimension\n",
    "- 4096 as dimension for the feed-forward layer\n",
    "- 0.1 as dropout value\n",
    "\n",
    "The original model is also enriched with a **T5-style relative positional encoding**, that is trained along with the three stage transformers. For this reimplementation, we adapted an implementation of the paper *\"Self-Attention with Relative Position Representations\"* provided by https://github.com/AliHaiderAhmad001/Self-Attention-with-Relative-Position-Representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3753f38-4d04-4cbb-9dfe-19815a11a0f5",
   "metadata": {},
   "source": [
    "### Semantic Transformer: expanding a sentence snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4c8f9-cf02-45a5-956b-ebae40f7d5e1",
   "metadata": {},
   "source": [
    "<center><img src=\"reportImg/semantic.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b14089-52c2-4296-a030-83f3fa2775fd",
   "metadata": {},
   "source": [
    "The first transformer is responsible for the generation of new semantic tokens, estimating $ p(z_{t}|z_{\\lt t}) $, where $ z_{i} $ denotes the $i$-th semantic token. In this way, given the semantic aspects of the prompt, the network is able to generate **new coherent content**, keeping long-term consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2061771-98ce-44f9-9cc7-3f68a742b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98dcc5e3-3a3e-4449-98d8-a2b7e8947626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found. Starting from scratch...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/semantic-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=15 #30\n",
    "vocab_size=500\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = SemanticTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = SemanticTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d09c68-8cd0-46e2-9439-78899c395d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming training...\")\n",
    "    # Pass the checkpoint path to trainer.fit\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    # Start training from scratch\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96786d9-b25b-499c-93f7-183ea82dc68f",
   "metadata": {},
   "source": [
    "### Coarse Transformer: generating new audio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed3993-72ab-4f88-8a94-5e9592e79e0a",
   "metadata": {},
   "source": [
    "<center><img src=\"reportImg/coarse.png\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f7266b-c51a-4a2a-80d1-3fedd1b7493b",
   "metadata": {},
   "source": [
    "Once we have defined a mechanism to generate new semantic content, we need to generate the corresponding acoustic content. Since we want the new tokens to be **coherent both with the sentence meaning** (represented by the Semantic tokens) and the **original speaker and enviroment conditions** (represented by the past Coarse Tokens), this model is trained to estimate:\n",
    "$$ p(y_t^q \\mid z, y_{<t}^{<Q'}, y_t^{<q})  \\quad \\text{for} \\ q \\le Q'$$\n",
    "where the current generated semantic token depends on **Semantic tokens** $z$ and the **acoustic tokens** $ y_{<t}^{<Q'} $ and $ y_t^{<q} $ that references respectively to **previous and current audio parts**. In the formula, $Q'$ denotes the **number of Soundstream quantizer** dedicated to Coarse tokens generation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bf990-df9a-4cd2-879b-1e2366fa1405",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0a13f-7cc6-4d72-9fb3-64932ddd02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/coarse-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=5 #10\n",
    "vocab_size=1024\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = CoarseTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = CoarseTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac8d88-f580-4454-a59d-dfb4942e1a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming training...\")\n",
    "    # Pass the checkpoint path to trainer.fit\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    # Start training from scratch\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab83b2dd-c6dc-4fe1-b4a9-3b1521921c76",
   "metadata": {},
   "source": [
    "### Fine Transformer: rebuilding a detailed audio waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d4d86-bb63-43dc-b456-04493ea4d9d8",
   "metadata": {},
   "source": [
    "<center><img src=\"reportImg/fine.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f651d-2e85-4c76-9a01-d68439c5887d",
   "metadata": {},
   "source": [
    "At last, we need to generate the **acoustic details** of the extended audio, since we need to recreate the **hierarchical structure** of acoustic features needed for the SoundStream model. This is easily done through a third and last transformer, that given the Coarse tokens and the previous Fine tokens, is trained to generate new Fine tokens. In formula:\n",
    "\n",
    "$$ p(y_t^q \\mid y_{<t}^{\\leq Q'}, y_{\\geq t}^{> Q'}, y_t^{<q}) \\quad \\text{for} \\ q > Q' $$\n",
    "\n",
    "where the new tokens depends on the **Coarse tokens** $y_{<t}^{\\leq Q'}$ and the **Fine tokens** $y_{\\geq t}^{> Q'}$ and $y_t^{<q}$, belonging respectively to **previous and current audio parts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff43e87-43c8-45a0-98c7-bca4615c212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_PERCENTAGE = 0.8\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [TRAINING_PERCENTAGE, 1 - TRAINING_PERCENTAGE])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    log_every_n_steps=1,\n",
    "    #devices=1 if torch.cuda.is_available() else None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbdc491-ae02-438b-a18f-97c149be721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "checkpoint_path = \"checkpoints/fine-checkpoint-10h-colab.ckpt\"\n",
    "d_model=256 #1024\n",
    "num_layers=3 #12\n",
    "num_heads=4 #16\n",
    "dim_feedforward=1024 #4096\n",
    "audioDuration=1.5 #3\n",
    "vocab_size=1024\n",
    "\n",
    "\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming old model...\")\n",
    "    model = FineTransformer.load_from_checkpoint(checkpoint_path, d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    model = FineTransformer(d_model=d_model, num_layers = num_layers, num_heads=num_heads, k = int(d_model/num_heads), dim_feedforward=dim_feedforward, audioDuration = audioDuration, vocab_size = vocab_size, myDevice = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91fd886-e05f-4da2-bcea-75b57a614262",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint found at {checkpoint_path}. Resuming training...\")\n",
    "    # Pass the checkpoint path to trainer.fit\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader, ckpt_path=checkpoint_path)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting from scratch...\")\n",
    "    # Start training from scratch\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc15ab-c234-4aa0-b039-bf528c361d03",
   "metadata": {},
   "source": [
    "Once we have generated both Coarse acoustic tokens and Fine acoustic tokens, we can reshape them into a format that resembles the original data provided by the Residual Vector Quantizer component of SoundStream. Finally we pass the Acoustic tokens into the Decoder part of Soundstream, that will return the final audio waveform.\n",
    "\n",
    "<center><img src=\"reportImg/full.png\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e839fc-5c63-4c15-b030-99e079b38380",
   "metadata": {},
   "source": [
    "## Inference and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e5882-33aa-4899-8210-965c1d11f31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b202049-fa9d-4f90-85bf-3897218d6ff0",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369a4e1f-cbf6-4f8b-b2ca-c9fcdca8506e",
   "metadata": {},
   "source": [
    "- AudioLM paper: https://arxiv.org/abs/2209.03143\n",
    "- AudioLM blog post: https://research.google/blog/audiolm-a-language-modeling-approach-to-audio-generation/\n",
    "- Relative positional embeddings implementation: https://github.com/AliHaiderAhmad001/Self-Attention-with-Relative-Position-Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba20c9c-4fb7-42df-9fc9-63b8adc75d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
