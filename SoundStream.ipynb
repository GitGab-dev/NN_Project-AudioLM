{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14-HyFoFbBRL",
        "outputId": "0c6ba778-02a2-470c-941c-874a9508f222"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHpsyjv4L5_Z"
      },
      "outputs": [],
      "source": [
        "from typing import Tuple, List, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ImportError:\n",
        "    class pl:\n",
        "        class LightningModule:\n",
        "            pass\n",
        "\n",
        "        class Callback:\n",
        "            pass\n",
        "from itertools import chain\n",
        "import random\n",
        "import torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cNttW-3UqKi"
      },
      "source": [
        "#SoundStream Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA81gSbTUXVm"
      },
      "source": [
        "##Residual Unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiUnQaQgMJQP"
      },
      "outputs": [],
      "source": [
        "class ResNet1d(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels,\n",
        "        kernel_size: int = 7,\n",
        "        padding: str = 'valid',\n",
        "        dilation: int = 1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self._padding_size = (kernel_size // 2) * dilation\n",
        "        self.conv0 = nn.Conv1d(\n",
        "            n_channels,\n",
        "            n_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            dilation=dilation)\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            n_channels,\n",
        "            n_channels,\n",
        "            kernel_size=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        y = input\n",
        "        x = self.conv0(input)\n",
        "        x = F.elu(x)\n",
        "        x = self.conv1(x)\n",
        "        if self.padding == 'valid':\n",
        "            y = y[:, :, self._padding_size:-self._padding_size]\n",
        "        x += y\n",
        "        x = F.elu(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT1dshhaUb1v"
      },
      "source": [
        "##Encoder Unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXoL8Sk4MOUg"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int,\n",
        "        padding: str,\n",
        "        stride: int\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=1),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=3),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=9),\n",
        "            nn.Conv1d(\n",
        "                n_channels // 2, n_channels,\n",
        "                kernel_size=2 * stride,\n",
        "                padding=(2 * stride) // 2 if padding == 'same' else 0,\n",
        "                stride=stride),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_channels: int, padding):\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv1d(1, n_channels, kernel_size=7, padding=padding),\n",
        "            nn.ELU(),\n",
        "            EncoderBlock(2 * n_channels, padding=padding, stride=2),\n",
        "            EncoderBlock(4 * n_channels, padding=padding, stride=4),\n",
        "            EncoderBlock(8 * n_channels, padding=padding, stride=5),\n",
        "            EncoderBlock(16 * n_channels, padding=padding, stride=8),\n",
        "            nn.Conv1d(16 * n_channels, 16 * n_channels, kernel_size=3, padding=padding),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejH9K-PFUfsp"
      },
      "source": [
        "##Decoder Unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrIJHw1jNXK5"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int,\n",
        "        padding: str,\n",
        "        stride: int\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.ConvTranspose1d(\n",
        "                n_channels, n_channels // 2,\n",
        "                kernel_size=2 * stride,\n",
        "                padding=(2 * stride) // 2 if padding == 'same' else 0,\n",
        "                stride=stride),\n",
        "            nn.ELU(),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=1),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=3),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=9),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_channels: int, padding):\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv1d(16 * n_channels, 16 * n_channels, kernel_size=7, padding=padding),\n",
        "            nn.ELU(),\n",
        "            DecoderBlock(16 * n_channels, padding=padding, stride=8),\n",
        "            DecoderBlock(8 * n_channels, padding=padding, stride=5),\n",
        "            DecoderBlock(4 * n_channels, padding=padding, stride=4),\n",
        "            DecoderBlock(2 * n_channels, padding=padding, stride=2),\n",
        "            nn.Conv1d(n_channels, 1, kernel_size=7, padding=padding),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10M79mLnUicz"
      },
      "source": [
        "##Residual Vector Quantizer Unit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jd9fZmlLa84V"
      },
      "outputs": [],
      "source": [
        "class ResidualVectorQuantizer(nn.Module):\n",
        "    weight: torch.Tensor\n",
        "    running_mean: torch.Tensor\n",
        "    code_count: torch.Tensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_quantizers: int,\n",
        "        num_embeddings: int,\n",
        "        embedding_dim: int,\n",
        "        decay: float = 0.99,\n",
        "        code_replace_threshold: float = 0.0001,\n",
        "        eps: float = 1e-10,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.num_quantizers = num_quantizers\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.register_buffer(\"weight\", torch.empty(num_quantizers, num_embeddings, embedding_dim))\n",
        "        self.register_buffer(\"running_mean\", torch.empty(num_quantizers, num_embeddings, embedding_dim))\n",
        "        self.register_buffer(\"code_count\", torch.empty(num_quantizers, num_embeddings))\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.code_replace_threshold = code_replace_threshold\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        init.uniform_(self.weight)\n",
        "        self.running_mean[:] = self.weight\n",
        "        init.ones_(self.code_count)\n",
        "\n",
        "    @torch.cuda.amp.autocast(enabled=False)\n",
        "    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
        "        # input: [..., chennel]\n",
        "        if self.training:\n",
        "            # Enabling bitrate scalability with quantizer dropout\n",
        "            n = random.randrange(1, self.num_quantizers)\n",
        "        else:\n",
        "            n = self.num_quantizers\n",
        "        codes = []\n",
        "        r = input.type_as(self.running_mean).detach()\n",
        "        with torch.no_grad():\n",
        "            for i in range(n):\n",
        "                w = self.weight[i]\n",
        "                # r: [..., num_embeddings]\n",
        "                dist = torch.cdist(r, w)\n",
        "                k = torch.argmin(dist, axis=-1)\n",
        "                codes.append(k)\n",
        "                self._update_averages(i, r, k)\n",
        "                r = r - F.embedding(k, w)\n",
        "        quantized = input - r\n",
        "        commitment_loss = torch.mean(torch.square(input - quantized.detach()))\n",
        "        self.weight.data[:] = self.running_mean / torch.unsqueeze(self.eps + self.code_count, axis=-1)\n",
        "        return quantized, torch.stack(codes, input.ndim - 1), commitment_loss\n",
        "\n",
        "    def dequantize(self, input: torch.Tensor, n: Optional[int] = None) -> torch.Tensor:\n",
        "        # input: [batch_size, length, num_quantizers]\n",
        "        if n is None:\n",
        "            n = input.shape[-1]\n",
        "        assert 0 < n <= self.num_quantizers\n",
        "        res = 0\n",
        "        with torch.no_grad():\n",
        "            for i in range(n):\n",
        "                k = input[:, :, i]\n",
        "                w = self.weight[i]\n",
        "                res += F.embedding(k, w)\n",
        "        return res\n",
        "\n",
        "    def _update_averages(self, i: int, r: torch.Tensor, k: torch.Tensor) -> None:\n",
        "        # https://arxiv.org/pdf/1906.00446.pdf\n",
        "        # Generating Diverse High-Fidelity Images with VQ-VAE-2\n",
        "        # 2.1 Vector Quantized Variational AutoEncode\n",
        "\n",
        "        # k: [...]\n",
        "        one_hot_k = F.one_hot(torch.flatten(k), self.num_embeddings).type_as(self.code_count)\n",
        "        code_count_update = torch.mean(one_hot_k, axis=0)\n",
        "        self.code_count[i].lerp_(code_count_update, 1 - self.decay)\n",
        "\n",
        "        # r: [..., embedding_dim]\n",
        "        r = r.reshape(-1, self.embedding_dim)\n",
        "        running_mean_update = (one_hot_k.T @ r) / r.shape[0]\n",
        "        running_mean_update = running_mean_update.to(self.running_mean[i].dtype)\n",
        "\n",
        "        self.running_mean[i].lerp_(running_mean_update, 1 - self.decay)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    @torch.cuda.amp.autocast(enabled=False)\n",
        "    def replace_vectors(self) -> int:\n",
        "        # https://arxiv.org/pdf/2107.03312.pdf\n",
        "        # C. Residual Vector Quantizer:\n",
        "\n",
        "        # The original paper replaces with an input frame randomly\n",
        "        # sampled within the current batch.\n",
        "        # Here we replace with random average of running mean instead.\n",
        "        num_replaced = torch.sum(self.code_count < self.code_replace_threshold).item()\n",
        "        if num_replaced > 0:\n",
        "            for i in range(self.num_quantizers):\n",
        "                mask = self.code_count[i] < self.code_replace_threshold\n",
        "                # mask: [num_quantizers, num_embeddings]\n",
        "                w = torch.rand_like(self.code_count[i])\n",
        "                w /= torch.sum(w)\n",
        "                self.running_mean[i, mask] = w.type_as(self.running_mean) @ self.running_mean[i]\n",
        "                self.code_count[i, mask] = w.type_as(self.code_count) @ self.code_count[i]\n",
        "\n",
        "        return num_replaced\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def calc_entropy(self) -> float:\n",
        "        p = self.code_count / (self.eps + torch.sum(self.code_count, axis=-1, keepdim=True))\n",
        "        return -torch.sum(torch.log(p) * p).item() / self.num_quantizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-YwalWDUyUG"
      },
      "source": [
        "#Pre-Trained Model Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_GAycevkPy5"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int = 32,\n",
        "        num_quantizers: int = 8,\n",
        "        num_embeddings: int = 1024,\n",
        "        padding: str = \"same\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(n_channels, padding)\n",
        "        self.decoder = Decoder(n_channels, padding)\n",
        "        self.quantizer = ResidualVectorQuantizer(\n",
        "            num_quantizers, num_embeddings, n_channels * 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "    def encode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        assert input.ndim == 2\n",
        "        x = torch.unsqueeze(input, 1)\n",
        "        x = self.encoder(x)\n",
        "        x = torch.transpose(x, -1, -2)\n",
        "        print(x.shape)\n",
        "        _, codes, _ = self.quantizer(x)\n",
        "        return codes\n",
        "\n",
        "    def decode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        # input: [batch_size, length, num_quantizers]\n",
        "        x = self.quantizer.dequantize(input)\n",
        "        x = torch.transpose(x, -1, -2)\n",
        "        x = self.decoder(x)\n",
        "        x = torch.squeeze(x, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def soundstream_16khz(pretrained=False, **kwargs):\n",
        "    \"\"\"SoundStream encoder decoder\n",
        "\n",
        "    pretrained (bool): kwargs, load pretrained weights into the model\n",
        "    \"\"\"\n",
        "    # Call the model, load pretrained weights\n",
        "    model = EncoderDecoder()\n",
        "    state_dict = torch.hub.load_state_dict_from_url(\"https://github.com/kaiidams/soundstream-pytorch/releases/download/v1.0/soundstream_16khz-20230425.ckpt\", map_location='cpu')\n",
        "    model.load_state_dict(state_dict['state_dict'], strict=False)\n",
        "    model.eval()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r974uuhLcKJv",
        "outputId": "958265f3-811a-435f-bd3d-c7bf1f2484f7"
      },
      "outputs": [],
      "source": [
        "model = soundstream_16khz()\n",
        "x, sr = torchaudio.load('canterburytales_09_chaucer_64kb_0000.flac')\n",
        "x, sr = torchaudio.functional.resample(x, sr, 16000), 16000\n",
        "\n",
        "#x = x[:, 0:5*16000]\n",
        "with torch.no_grad():\n",
        "    y = model.encode(x)\n",
        "    #y = y[:, :, :4]  # if you want to reduce code size.\n",
        "    z = model.decode(y)\n",
        "#torchaudio.save('output.flac', z, sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GLqL0uYLgN8i"
      },
      "outputs": [],
      "source": [
        "torchaudio.save('output.flac', z, sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKMmwDLGdBlr"
      },
      "outputs": [],
      "source": [
        "r, sr = torchaudio.load('canterburytales_09_chaucer_64kb_0000.flac')\n",
        "r, sr = torchaudio.functional.resample(r, sr, 16000), 16000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4c0j9c2eepC",
        "outputId": "e39eab3f-1139-494d-be89-c06faa7a4c78"
      },
      "outputs": [],
      "source": [
        "print(z.shape)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "232fb939c2274f0b94b7c1705a4e9136": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ec194a180ec4962b13e5ab9c6e96a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5308e80c63e7475c9cbfc37d448bafff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "564b3c8322a949bd96873051319b4a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_565c493e7073496a9ba9b92e284e4119",
            "placeholder": "​",
            "style": "IPY_MODEL_232fb939c2274f0b94b7c1705a4e9136",
            "value": "Epoch 0:   0%"
          }
        },
        "565c493e7073496a9ba9b92e284e4119": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57533b434bb84b5dba280aaa44277bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a3562cc4b1e4d538901ebb04d4e918e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5fe0e7801f124e7b8db8452e93cb501f",
            "max": 892,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_57533b434bb84b5dba280aaa44277bff",
            "value": 0
          }
        },
        "5fe0e7801f124e7b8db8452e93cb501f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9134241e764ccbb53e5cc5919a48e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d96b135969f4e3cbf289f10a8b690f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c9134241e764ccbb53e5cc5919a48e6",
            "placeholder": "​",
            "style": "IPY_MODEL_2ec194a180ec4962b13e5ab9c6e96a82",
            "value": " 0/892 [00:00&lt;?, ?it/s]"
          }
        },
        "f9019f0c5de040e08e478e1a67aa6376": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_564b3c8322a949bd96873051319b4a5f",
              "IPY_MODEL_5a3562cc4b1e4d538901ebb04d4e918e",
              "IPY_MODEL_7d96b135969f4e3cbf289f10a8b690f5"
            ],
            "layout": "IPY_MODEL_5308e80c63e7475c9cbfc37d448bafff"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
