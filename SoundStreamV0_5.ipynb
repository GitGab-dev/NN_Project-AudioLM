{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6c9571b166204de2901dffea700f0982": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe7bc48dfd845abb1712241790ee8d5",
              "IPY_MODEL_743eeee7a1fe491da835ab8a18ef0b46",
              "IPY_MODEL_0f9ed14569e848a2816ced8a6564d219"
            ],
            "layout": "IPY_MODEL_624c7dd1a1ec432383074f0c6430dde2"
          }
        },
        "6fe7bc48dfd845abb1712241790ee8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d91a778a99304eec9372f1011267f169",
            "placeholder": "​",
            "style": "IPY_MODEL_c461c00ab79e412e836bf4780a451cdb",
            "value": "Epoch 0:   0%"
          }
        },
        "743eeee7a1fe491da835ab8a18ef0b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04dd94e29f0f436cb9bb40c418beb336",
            "max": 892,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71f598a998a344bf8052883cb9e141f7",
            "value": 0
          }
        },
        "0f9ed14569e848a2816ced8a6564d219": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e6e8c4f6f484fe2bee0fd360998f171",
            "placeholder": "​",
            "style": "IPY_MODEL_7ea787a8523c4e92b61f800701123abd",
            "value": " 0/892 [00:00&lt;?, ?it/s]"
          }
        },
        "624c7dd1a1ec432383074f0c6430dde2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "d91a778a99304eec9372f1011267f169": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c461c00ab79e412e836bf4780a451cdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04dd94e29f0f436cb9bb40c418beb336": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f598a998a344bf8052883cb9e141f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e6e8c4f6f484fe2bee0fd360998f171": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ea787a8523c4e92b61f800701123abd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14-HyFoFbBRL",
        "outputId": "faf960ae-2c2d-41d2-e399-a382b8bac2f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.66.4)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (2023.6.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (1.4.0.post0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch_lightning) (0.11.3.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (3.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch_lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pytorch_lightning) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->pytorch_lightning) (12.5.40)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pytorch_lightning) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.0->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torch==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchaudio) (2.3.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.15.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->torchaudio) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchaudio) (12.5.40)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->torchaudio) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->torchaudio) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple, List, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ImportError:\n",
        "    class pl:\n",
        "        class LightningModule:\n",
        "            pass\n",
        "\n",
        "        class Callback:\n",
        "            pass\n",
        "from itertools import chain\n",
        "import random\n",
        "import torchaudio"
      ],
      "metadata": {
        "id": "lHpsyjv4L5_Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SoundStream Model"
      ],
      "metadata": {
        "id": "8cNttW-3UqKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Residual Unit"
      ],
      "metadata": {
        "id": "AA81gSbTUXVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet1d(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels,\n",
        "        kernel_size: int = 7,\n",
        "        padding: str = 'valid',\n",
        "        dilation: int = 1\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.dilation = dilation\n",
        "        self._padding_size = (kernel_size // 2) * dilation\n",
        "        self.conv0 = nn.Conv1d(\n",
        "            n_channels,\n",
        "            n_channels,\n",
        "            kernel_size=kernel_size,\n",
        "            padding=padding,\n",
        "            dilation=dilation)\n",
        "        self.conv1 = nn.Conv1d(\n",
        "            n_channels,\n",
        "            n_channels,\n",
        "            kernel_size=1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        y = input\n",
        "        x = self.conv0(input)\n",
        "        x = F.elu(x)\n",
        "        x = self.conv1(x)\n",
        "        if self.padding == 'valid':\n",
        "            y = y[:, :, self._padding_size:-self._padding_size]\n",
        "        x += y\n",
        "        x = F.elu(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DiUnQaQgMJQP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encoder Unit"
      ],
      "metadata": {
        "id": "uT1dshhaUb1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int,\n",
        "        padding: str,\n",
        "        stride: int\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=1),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=3),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=9),\n",
        "            nn.Conv1d(\n",
        "                n_channels // 2, n_channels,\n",
        "                kernel_size=2 * stride,\n",
        "                padding=(2 * stride) // 2 if padding == 'same' else 0,\n",
        "                stride=stride),\n",
        "            nn.ELU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_channels: int, padding):\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv1d(1, n_channels, kernel_size=7, padding=padding),\n",
        "            nn.ELU(),\n",
        "            EncoderBlock(2 * n_channels, padding=padding, stride=2),\n",
        "            EncoderBlock(4 * n_channels, padding=padding, stride=4),\n",
        "            EncoderBlock(8 * n_channels, padding=padding, stride=5),\n",
        "            EncoderBlock(16 * n_channels, padding=padding, stride=8),\n",
        "            nn.Conv1d(16 * n_channels, 16 * n_channels, kernel_size=3, padding=padding),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)"
      ],
      "metadata": {
        "id": "EXoL8Sk4MOUg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decoder Unit"
      ],
      "metadata": {
        "id": "ejH9K-PFUfsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int,\n",
        "        padding: str,\n",
        "        stride: int\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.ConvTranspose1d(\n",
        "                n_channels, n_channels // 2,\n",
        "                kernel_size=2 * stride,\n",
        "                padding=(2 * stride) // 2 if padding == 'same' else 0,\n",
        "                stride=stride),\n",
        "            nn.ELU(),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=1),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=3),\n",
        "            ResNet1d(n_channels // 2, padding=padding, dilation=9),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_channels: int, padding):\n",
        "        super().__init__()\n",
        "        assert padding in ['valid', 'same']\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv1d(16 * n_channels, 16 * n_channels, kernel_size=7, padding=padding),\n",
        "            nn.ELU(),\n",
        "            DecoderBlock(16 * n_channels, padding=padding, stride=8),\n",
        "            DecoderBlock(8 * n_channels, padding=padding, stride=5),\n",
        "            DecoderBlock(4 * n_channels, padding=padding, stride=4),\n",
        "            DecoderBlock(2 * n_channels, padding=padding, stride=2),\n",
        "            nn.Conv1d(n_channels, 1, kernel_size=7, padding=padding),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        return self.layers(input)"
      ],
      "metadata": {
        "id": "WrIJHw1jNXK5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Residual Vector Quantizer Unit"
      ],
      "metadata": {
        "id": "10M79mLnUicz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Jd9fZmlLa84V"
      },
      "outputs": [],
      "source": [
        "class ResidualVectorQuantizer(nn.Module):\n",
        "    weight: torch.Tensor\n",
        "    running_mean: torch.Tensor\n",
        "    code_count: torch.Tensor\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_quantizers: int,\n",
        "        num_embeddings: int,\n",
        "        embedding_dim: int,\n",
        "        decay: float = 0.99,\n",
        "        code_replace_threshold: float = 0.0001,\n",
        "        eps: float = 1e-10,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.num_quantizers = num_quantizers\n",
        "        self.num_embeddings = num_embeddings\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.register_buffer(\"weight\", torch.empty(num_quantizers, num_embeddings, embedding_dim))\n",
        "        self.register_buffer(\"running_mean\", torch.empty(num_quantizers, num_embeddings, embedding_dim))\n",
        "        self.register_buffer(\"code_count\", torch.empty(num_quantizers, num_embeddings))\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        self.code_replace_threshold = code_replace_threshold\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self) -> None:\n",
        "        init.uniform_(self.weight)\n",
        "        self.running_mean[:] = self.weight\n",
        "        init.ones_(self.code_count)\n",
        "\n",
        "    @torch.cuda.amp.autocast(enabled=False)\n",
        "    def forward(self, input: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, List[torch.Tensor]]:\n",
        "        # input: [..., chennel]\n",
        "        if self.training:\n",
        "            # Enabling bitrate scalability with quantizer dropout\n",
        "            n = random.randrange(1, self.num_quantizers)\n",
        "        else:\n",
        "            n = self.num_quantizers\n",
        "        codes = []\n",
        "        r = input.type_as(self.running_mean).detach()\n",
        "        with torch.no_grad():\n",
        "            for i in range(n):\n",
        "                w = self.weight[i]\n",
        "                # r: [..., num_embeddings]\n",
        "                dist = torch.cdist(r, w)\n",
        "                k = torch.argmin(dist, axis=-1)\n",
        "                codes.append(k)\n",
        "                self._update_averages(i, r, k)\n",
        "                r = r - F.embedding(k, w)\n",
        "        quantized = input - r\n",
        "        commitment_loss = torch.mean(torch.square(input - quantized.detach()))\n",
        "        self.weight.data[:] = self.running_mean / torch.unsqueeze(self.eps + self.code_count, axis=-1)\n",
        "        return quantized, torch.stack(codes, input.ndim - 1), commitment_loss\n",
        "\n",
        "    def dequantize(self, input: torch.Tensor, n: Optional[int] = None) -> torch.Tensor:\n",
        "        # input: [batch_size, length, num_quantizers]\n",
        "        if n is None:\n",
        "            n = input.shape[-1]\n",
        "        assert 0 < n <= self.num_quantizers\n",
        "        res = 0\n",
        "        with torch.no_grad():\n",
        "            for i in range(n):\n",
        "                k = input[:, :, i]\n",
        "                w = self.weight[i]\n",
        "                res += F.embedding(k, w)\n",
        "        return res\n",
        "\n",
        "    def _update_averages(self, i: int, r: torch.Tensor, k: torch.Tensor) -> None:\n",
        "        # https://arxiv.org/pdf/1906.00446.pdf\n",
        "        # Generating Diverse High-Fidelity Images with VQ-VAE-2\n",
        "        # 2.1 Vector Quantized Variational AutoEncode\n",
        "\n",
        "        # k: [...]\n",
        "        one_hot_k = F.one_hot(torch.flatten(k), self.num_embeddings).type_as(self.code_count)\n",
        "        code_count_update = torch.mean(one_hot_k, axis=0)\n",
        "        self.code_count[i].lerp_(code_count_update, 1 - self.decay)\n",
        "\n",
        "        # r: [..., embedding_dim]\n",
        "        r = r.reshape(-1, self.embedding_dim)\n",
        "        running_mean_update = (one_hot_k.T @ r) / r.shape[0]\n",
        "        self.running_mean[i].lerp_(running_mean_update, 1 - self.decay)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    @torch.cuda.amp.autocast(enabled=False)\n",
        "    def replace_vectors(self) -> int:\n",
        "        # https://arxiv.org/pdf/2107.03312.pdf\n",
        "        # C. Residual Vector Quantizer:\n",
        "\n",
        "        # The original paper replaces with an input frame randomly\n",
        "        # sampled within the current batch.\n",
        "        # Here we replace with random average of running mean instead.\n",
        "        num_replaced = torch.sum(self.code_count < self.code_replace_threshold).item()\n",
        "        if num_replaced > 0:\n",
        "            for i in range(self.num_quantizers):\n",
        "                mask = self.code_count[i] < self.code_replace_threshold\n",
        "                # mask: [num_quantizers, num_embeddings]\n",
        "                w = torch.rand_like(self.code_count[i])\n",
        "                w /= torch.sum(w)\n",
        "                self.running_mean[i, mask] = w.type_as(self.running_mean) @ self.running_mean[i]\n",
        "                self.code_count[i, mask] = w.type_as(self.code_count) @ self.code_count[i]\n",
        "\n",
        "        return num_replaced\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def calc_entropy(self) -> float:\n",
        "        p = self.code_count / (self.eps + torch.sum(self.code_count, axis=-1, keepdim=True))\n",
        "        return -torch.sum(torch.log(p) * p).item() / self.num_quantizers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "oxz6rdyeUnOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet2d(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int,\n",
        "        factor: int,\n",
        "        stride: Tuple[int, int]\n",
        "    ) -> None:\n",
        "        # https://arxiv.org/pdf/2005.00341.pdf\n",
        "        # The original paper uses layer normalization, but here\n",
        "        # we use batch normalization.\n",
        "        super().__init__()\n",
        "        self.conv0 = nn.Conv2d(\n",
        "            n_channels,\n",
        "            n_channels,\n",
        "            kernel_size=(3, 3),\n",
        "            padding='same')\n",
        "        self.bn0 = nn.BatchNorm2d(\n",
        "            n_channels\n",
        "        )\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            n_channels,\n",
        "            factor * n_channels,\n",
        "            kernel_size=(stride[0] + 2, stride[1] + 2),\n",
        "            stride=stride)\n",
        "        self.bn1 = nn.BatchNorm2d(\n",
        "            factor * n_channels\n",
        "        )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            n_channels,\n",
        "            factor * n_channels,\n",
        "            kernel_size=1,\n",
        "            stride=stride)\n",
        "        self.bn2 = nn.BatchNorm2d(\n",
        "            factor * n_channels\n",
        "        )\n",
        "        self.pad = nn.ReflectionPad2d([\n",
        "            (stride[1] + 1) // 2,\n",
        "            (stride[1] + 2) // 2,\n",
        "            (stride[0] + 1) // 2,\n",
        "            (stride[0] + 2) // 2,\n",
        "        ])\n",
        "        self.activation = nn.LeakyReLU(0.3)\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.conv0(input)\n",
        "        x = self.bn0(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.pad(x)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "\n",
        "        # shortcut\n",
        "        y = self.conv2(input)\n",
        "        y = self.bn2(y)\n",
        "\n",
        "        x += y\n",
        "        x = self.activation(x)\n",
        "        return x\n",
        "\n",
        "class WaveDiscriminator(nn.Module):\n",
        "    r\"\"\"MelGAN discriminator from https://arxiv.org/pdf/1910.06711.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, resolution: int = 1, n_channels: int = 4) -> None:\n",
        "        super().__init__()\n",
        "        assert resolution >= 1\n",
        "        if resolution == 1:\n",
        "            self.avg_pool = nn.Identity()\n",
        "        else:\n",
        "            self.avg_pool = nn.AvgPool1d(resolution * 2, stride=resolution)\n",
        "        self.activation = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.layers = nn.ModuleList([\n",
        "            nn.utils.weight_norm(nn.Conv1d(1, n_channels, kernel_size=15, padding=7)),\n",
        "            nn.utils.weight_norm(nn.Conv1d(n_channels, 4 * n_channels, kernel_size=41, stride=4, padding=20, groups=4)),\n",
        "            nn.utils.weight_norm(nn.Conv1d(4 * n_channels, 16 * n_channels, kernel_size=41, stride=4, padding=20, groups=16)),\n",
        "            nn.utils.weight_norm(nn.Conv1d(16 * n_channels, 64 * n_channels, kernel_size=41, stride=4, padding=20, groups=64)),\n",
        "            nn.utils.weight_norm(nn.Conv1d(64 * n_channels, 256 * n_channels, kernel_size=41, stride=4, padding=20, groups=256)),\n",
        "            nn.utils.weight_norm(nn.Conv1d(256 * n_channels, 256 * n_channels, kernel_size=5, padding=2)),\n",
        "            nn.utils.weight_norm(nn.Conv1d(256 * n_channels, 1, kernel_size=3, padding=1)),\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:\n",
        "        x = self.avg_pool(x)\n",
        "        feats = []\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = layer(x)\n",
        "            feats.append(x)\n",
        "            x = self.activation(x)\n",
        "        feats.append(self.layers[-1](x))\n",
        "        return feats\n",
        "\n",
        "\n",
        "class STFTDiscriminator(nn.Module):\n",
        "    r\"\"\"STFT-based discriminator from https://arxiv.org/pdf/2107.03312.pdf\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, n_fft: int = 1024, hop_length: int = 256,\n",
        "        n_channels: int = 32\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        n = n_fft // 2 + 1\n",
        "        for _ in range(6):\n",
        "            n = (n - 1) // 2 + 1\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Conv2d(1, n_channels, kernel_size=7, padding='same'),\n",
        "            nn.LeakyReLU(0.3, inplace=True),\n",
        "            ResNet2d(n_channels, 2, stride=(2, 1)),\n",
        "            ResNet2d(2 * n_channels, 2, stride=(2, 2)),\n",
        "            ResNet2d(4 * n_channels, 1, stride=(2, 1)),\n",
        "            ResNet2d(4 * n_channels, 2, stride=(2, 2)),\n",
        "            ResNet2d(8 * n_channels, 1, stride=(2, 1)),\n",
        "            ResNet2d(8 * n_channels, 2, stride=(2, 2)),\n",
        "            nn.Conv2d(16 * n_channels, 1, kernel_size=(n, 1))\n",
        "        )\n",
        "\n",
        "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        assert input.shape[1] == 1\n",
        "        # input: [batch, channel, sequence]\n",
        "        x = torch.squeeze(input, 1).to(torch.float32)  # torch.stft() doesn't accept float16\n",
        "        x = torch.stft(x, self.n_fft, self.hop_length, normalized=True, onesided=True, return_complex=True)\n",
        "        x = torch.abs(x)\n",
        "        x = torch.unsqueeze(x, dim=1)\n",
        "        x = self.layers(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ReconstructionLoss(nn.Module):\n",
        "    \"\"\"Reconstruction loss from https://arxiv.org/pdf/2107.03312.pdf\n",
        "    but uses STFT instead of mel-spectrogram\n",
        "    \"\"\"\n",
        "    def __init__(self, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss = 0\n",
        "        input = input.to(torch.float32)\n",
        "        target = target.to(torch.float32)\n",
        "        for i in range(6, 12):\n",
        "            s = 2 ** i\n",
        "            alpha = (s / 2) ** 0.5\n",
        "            # We use STFT instead of 64-bin mel-spectrogram as n_fft=64 is too small\n",
        "            # for 64 bins.\n",
        "            x = torch.stft(input, n_fft=s, hop_length=s // 4, win_length=s, normalized=True, onesided=True, return_complex=True)\n",
        "            x = torch.abs(x)\n",
        "            y = torch.stft(target, n_fft=s, hop_length=s // 4, win_length=s, normalized=True, onesided=True, return_complex=True)\n",
        "            y = torch.abs(y)\n",
        "            if x.shape[-1] > y.shape[-1]:\n",
        "                x = x[:, :, :y.shape[-1]]\n",
        "            elif x.shape[-1] < y.shape[-1]:\n",
        "                y = y[:, :, :x.shape[-1]]\n",
        "            loss += torch.mean(torch.abs(x - y))\n",
        "            loss += alpha * torch.mean(torch.square(torch.log(x + self.eps) - torch.log(y + self.eps)))\n",
        "        return loss / (12 - 6)\n",
        "\n",
        "\n",
        "class ReconstructionLoss2(nn.Module):\n",
        "    \"\"\"Reconstruction loss from https://arxiv.org/pdf/2107.03312.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, sample_rate, eps=1e-5):\n",
        "        super().__init__()\n",
        "        import torchaudio\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.alpha = []\n",
        "        self.eps = eps\n",
        "        for i in range(6, 12):\n",
        "            melspec = torchaudio.transforms.MelSpectrogram(\n",
        "                sample_rate=sample_rate,\n",
        "                n_fft=int(2 ** i),\n",
        "                win_length=int(2 ** i),\n",
        "                hop_length=int(2 ** i / 4),\n",
        "                n_mels=64)\n",
        "            self.layers.append(melspec)\n",
        "            self.alpha.append((2 ** i / 2) ** 0.5)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        loss = 0\n",
        "        for alpha, melspec in zip(self.alpha, self.layers):\n",
        "            x = melspec(input)\n",
        "            y = melspec(target)\n",
        "            if x.shape[-1] > y.shape[-1]:\n",
        "                x = x[:, y.shape[-1]]\n",
        "            elif x.shape[-1] < y.shape[-1]:\n",
        "                y = y[:, x.shape[-1]]\n",
        "            loss += torch.mean(torch.abs(x - y))\n",
        "            loss += alpha * torch.mean(torch.square(torch.log(x + self.eps) - torch.log(y + self.eps)))\n",
        "        return loss\n",
        "\n",
        "class StreamableModel(pl.LightningModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int = 32,\n",
        "        num_quantizers: int = 8, #12 in the AudioLM Paper, 8 in the Pre-Trained Model\n",
        "        num_embeddings: int = 1024,\n",
        "        padding: str = \"valid\",\n",
        "        batch_size: int = 32,\n",
        "        sample_rate: int = 24_000,\n",
        "        segment_length: int = 32270,\n",
        "        lr: float = 1e-4,\n",
        "        b1: float = 0.5,\n",
        "        b2: float = 0.9,\n",
        "        dataset: str = 'librispeech'\n",
        "    ) -> None:\n",
        "        # https://arxiv.org/pdf/2009.02095.pdf\n",
        "        # 2. Method\n",
        "        # SEANet uses Adam with lr=1e-4, beta1=0.5, beta2=0.9\n",
        "        # batch_size=16\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.automatic_optimization = False\n",
        "        self.encoder = Encoder(n_channels, padding)\n",
        "        self.decoder = Decoder(n_channels, padding)\n",
        "        self.quantizer = ResidualVectorQuantizer(\n",
        "            num_quantizers, num_embeddings, n_channels * 16)\n",
        "\n",
        "        self.wave_discriminators = nn.ModuleList([\n",
        "            WaveDiscriminator(resolution=1),\n",
        "            WaveDiscriminator(resolution=2),\n",
        "            WaveDiscriminator(resolution=4)\n",
        "        ])\n",
        "        self.rec_loss = ReconstructionLoss()\n",
        "        self.stft_discriminator = STFTDiscriminator()\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        b1 = self.hparams.b1\n",
        "        b2 = self.hparams.b2\n",
        "\n",
        "        optimizer_g = torch.optim.Adam(\n",
        "            chain(\n",
        "                self.encoder.parameters(),\n",
        "                self.decoder.parameters()\n",
        "            ),\n",
        "            lr=lr, betas=(b1, b2))\n",
        "        optimizer_d = torch.optim.Adam(\n",
        "            chain(\n",
        "                self.wave_discriminators.parameters(),\n",
        "                self.stft_discriminator.parameters()\n",
        "            ),\n",
        "            lr=lr, betas=(b1, b2))\n",
        "        return [optimizer_g, optimizer_d], []\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.encoder(input)\n",
        "        x = torch.transpose(x, -1, -2)\n",
        "        x, codes, codebook_loss = self.quantizer(x)\n",
        "        x = torch.transpose(x, -1, -2)\n",
        "        x = self.decoder(x)\n",
        "        return x, codes, codebook_loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        optimizer_g, optimizer_d = self.optimizers()\n",
        "        input = batch[:, None, :]\n",
        "        # input: [batch, channel, sequence]\n",
        "\n",
        "        # train generator\n",
        "        self.toggle_optimizer(optimizer_g)\n",
        "        output, _, q_loss = self.forward(input)\n",
        "        # output: [batch, channel, sequence]\n",
        "        # print(input.shape, output.shape)\n",
        "\n",
        "        stft_out = self.stft_discriminator(output)\n",
        "        g_stft_loss = torch.mean(torch.relu(1 - stft_out))\n",
        "        self.log(\"g_stft_loss\", g_stft_loss)\n",
        "\n",
        "        g_wave_loss = 0\n",
        "        g_feat_loss = 0\n",
        "        for i in range(3):\n",
        "            feats1 = self.wave_discriminators[i](input)\n",
        "            feats2 = self.wave_discriminators[i](output)\n",
        "            assert len(feats1) == len(feats2)\n",
        "            g_wave_loss += torch.mean(torch.relu(1 - feats2[-1]))\n",
        "            g_feat_loss += sum(torch.mean(\n",
        "                torch.abs(f1 - f2))\n",
        "                for f1, f2 in zip(feats1[:-1], feats2[:-1])) / (len(feats1) - 1)\n",
        "        self.log(\"g_wave_loss\", g_wave_loss / 3)\n",
        "        self.log(\"g_feat_loss\", g_feat_loss / 3)\n",
        "\n",
        "        g_rec_loss = self.rec_loss(output[:, 0, :], input[:, 0, :])\n",
        "        self.log(\"g_rec_loss\", g_rec_loss, prog_bar=True)\n",
        "\n",
        "        g_feat_loss = g_feat_loss / 3\n",
        "        g_adv_loss = (g_stft_loss + g_wave_loss) / 4\n",
        "        g_loss = g_adv_loss + 100 * g_feat_loss + g_rec_loss\n",
        "        self.log(\"q_loss\", q_loss, prog_bar=True)\n",
        "        self.log(\"g_loss\", g_loss, prog_bar=True)\n",
        "\n",
        "        self.manual_backward(g_loss + q_loss)\n",
        "        optimizer_g.step()\n",
        "        optimizer_g.zero_grad()\n",
        "        self.untoggle_optimizer(optimizer_g)\n",
        "\n",
        "        codes_entropy = self.quantizer.calc_entropy()\n",
        "        self.log(\"codes_entropy\", codes_entropy, prog_bar=True)\n",
        "\n",
        "        # train discriminator\n",
        "        self.toggle_optimizer(optimizer_d)\n",
        "        output, _, _ = self.forward(input)\n",
        "\n",
        "        stft_out = self.stft_discriminator(input)\n",
        "        d_stft_loss = torch.mean(torch.relu(1 - stft_out))\n",
        "        stft_out = self.stft_discriminator(output)\n",
        "        d_stft_loss += torch.mean(torch.relu(1 + stft_out))\n",
        "\n",
        "        d_wave_loss = 0\n",
        "        for i in range(3):\n",
        "            feats = self.wave_discriminators[i](input)\n",
        "            d_wave_loss += torch.mean(torch.relu(1 - feats[-1]))\n",
        "            feats = self.wave_discriminators[i](output)\n",
        "            d_wave_loss += torch.mean(torch.relu(1 + feats[-1]))\n",
        "\n",
        "        d_loss = (d_stft_loss + d_wave_loss) / 4\n",
        "\n",
        "        self.log(\"d_stft_loss\", d_stft_loss)\n",
        "        self.log(\"d_wave_loss\", d_wave_loss / 3)\n",
        "\n",
        "        d_loss = (d_stft_loss + d_wave_loss) / 4\n",
        "        self.log(\"d_loss\", d_loss, prog_bar=True)\n",
        "\n",
        "        self.manual_backward(d_loss)\n",
        "        optimizer_d.step()\n",
        "        optimizer_d.zero_grad()\n",
        "        self.untoggle_optimizer(optimizer_d)\n",
        "\n",
        "        num_replaced = self.quantizer.replace_vectors()\n",
        "        self.log(\"num_replaced\", float(num_replaced), prog_bar=True)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return self._make_dataloader(True)\n",
        "\n",
        "    def _make_dataloader(self, train: bool):\n",
        "        import torchaudio\n",
        "\n",
        "        def collate(examples):\n",
        "            return torch.stack(examples)\n",
        "\n",
        "        class VoiceDataset(torch.utils.data.Dataset):\n",
        "            def __init__(self, dataset, sample_rate, segment_length):\n",
        "                self._dataset = dataset\n",
        "                self._sample_rate = sample_rate\n",
        "                self._segment_length = segment_length\n",
        "\n",
        "            def __getitem__(self, index):\n",
        "                import random\n",
        "                x, sample_rate, *_ = self._dataset[index]\n",
        "                x = torchaudio.functional.resample(x, sample_rate, self._sample_rate)\n",
        "                assert x.shape[0] == 1\n",
        "                x = torch.squeeze(x)\n",
        "                x *= 0.95 / torch.max(x)\n",
        "                assert x.dim() == 1\n",
        "                if x.shape[0] < self._segment_length:\n",
        "                    x = F.pad(x, [0, self._segment_length - x.shape[0]], \"constant\")\n",
        "                pos = random.randint(0, x.shape[0] - self._segment_length)\n",
        "                x = x[pos:pos + self._segment_length]\n",
        "                return x\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self._dataset)\n",
        "\n",
        "        if self.hparams.dataset == 'yesno':\n",
        "            ds = torchaudio.datasets.YESNO(\"./data\", download=True)\n",
        "        elif self.hparams.dataset == 'librispeech-dev':\n",
        "            ds = torchaudio.datasets.LIBRISPEECH(\"./data\", url=\"dev-clean\")\n",
        "        elif self.hparams.dataset == 'librispeech':\n",
        "            url = \"train-clean-100\" if train else \"dev-clean\"\n",
        "            ds = torchaudio.datasets.LIBRISPEECH(\"./data\", url=url, download = True)\n",
        "        else:\n",
        "            raise ValueError()\n",
        "        ds = VoiceDataset(ds, self.hparams.sample_rate, self.hparams.segment_length)\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            ds, batch_size=self.hparams['batch_size'], shuffle=True,\n",
        "            collate_fn=collate)\n",
        "        return loader\n",
        "\n",
        "\n",
        "class KMeanCodebookInitCallback(pl.Callback):\n",
        "    def on_fit_start(self, trainer, model):\n",
        "        # https://arxiv.org/pdf/2107.03312.pdf\n",
        "        # C. Residual Vector Quantizer\n",
        "        # run the k-means\n",
        "        # algorithm on the first training batch and use the learned\n",
        "        # centroids as initialization\n",
        "        batch = next(iter(model.train_dataloader()))\n",
        "        input = batch[:, None, :].to(model.device)\n",
        "        with torch.no_grad():\n",
        "            x = torch.flatten(model.encoder(input))\n",
        "            mean = torch.mean(x, axis=0)\n",
        "            std = torch.std(x, axis=0)\n",
        "            torch.nn.init.normal_(model.quantizer.weight, mean=mean, std=std)\n",
        "        print(f\"KMeanCodebookInitCallback {mean} {std}\")\n",
        "\n",
        "\n",
        "def train():\n",
        "    model = StreamableModel(\n",
        "        batch_size=32,\n",
        "        sample_rate=16_000,\n",
        "        segment_length=32270,\n",
        "        padding='same',\n",
        "        dataset='librispeech')\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=10000,\n",
        "        log_every_n_steps=2,\n",
        "        precision='16-mixed',\n",
        "        logger=pl.loggers.CSVLogger(\".\"),\n",
        "        # logger=pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"soundstream\"),\n",
        "        callbacks=[\n",
        "            pl.callbacks.ModelCheckpoint(save_last=True, every_n_train_steps=50000),\n",
        "            KMeanCodebookInitCallback(),\n",
        "        ],\n",
        "    )\n",
        "    trainer.fit(\n",
        "        model,\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "rWjXbWYQNxU2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained = train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686,
          "referenced_widgets": [
            "6c9571b166204de2901dffea700f0982",
            "6fe7bc48dfd845abb1712241790ee8d5",
            "743eeee7a1fe491da835ab8a18ef0b46",
            "0f9ed14569e848a2816ced8a6564d219",
            "624c7dd1a1ec432383074f0c6430dde2",
            "d91a778a99304eec9372f1011267f169",
            "c461c00ab79e412e836bf4780a451cdb",
            "04dd94e29f0f436cb9bb40c418beb336",
            "71f598a998a344bf8052883cb9e141f7",
            "8e6e8c4f6f484fe2bee0fd360998f171",
            "7ea787a8523c4e92b61f800701123abd"
          ]
        },
        "id": "aP8f7pySWKXX",
        "outputId": "459cfd04-3e22-42d7-f5e3-515623caedfd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "100%|██████████| 5.95G/5.95G [03:01<00:00, 35.2MB/s]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name                | Type                    | Params | Mode \n",
            "------------------------------------------------------------------------\n",
            "0 | encoder             | Encoder                 | 5.4 M  | train\n",
            "1 | decoder             | Decoder                 | 6.4 M  | train\n",
            "2 | quantizer           | ResidualVectorQuantizer | 0      | train\n",
            "3 | wave_discriminators | ModuleList              | 15.9 M | train\n",
            "4 | rec_loss            | ReconstructionLoss      | 0      | train\n",
            "5 | stft_discriminator  | STFTDiscriminator       | 5.6 M  | train\n",
            "------------------------------------------------------------------------\n",
            "33.3 M    Trainable params\n",
            "0         Non-trainable params\n",
            "33.3 M    Total params\n",
            "133.114   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeanCodebookInitCallback -0.002216061344370246 0.03749164938926697\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c9571b166204de2901dffea700f0982"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "expected dtype float for `end` but got dtype c10::BFloat16",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-87ebd5228315>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-92c1fed8b672>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    411\u001b[0m         ],\n\u001b[1;32m    412\u001b[0m     )\n\u001b[0;32m--> 413\u001b[0;31m     trainer.fit(\n\u001b[0m\u001b[1;32m    414\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainerStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRUNNING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         call._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_TunerExitException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mmodel_connected\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         )\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m# RUN THE TRAINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unexpected state {self.state}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    250\u001b[0m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautomatic_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                     \u001b[0mbatch_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_optimization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincrement_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/manual.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_run_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no loop to break at this level\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_run_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/optimization/manual.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# manually capture logged metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mtraining_step_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m  \u001b[0;31m# release the batch from memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# unused hook - call anyway for backward compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;31m# restore current_fx when nested context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/strategy.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_redirection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"training_step\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost_training_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-92c1fed8b672>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# train generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoggle_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_g\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0;31m# output: [batch, channel, sequence]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# print(input.shape, output.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-92c1fed8b672>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodebook_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9ab065743396>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mcodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_averages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mquantized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-9ab065743396>\u001b[0m in \u001b[0;36m_update_averages\u001b[0;34m(self, i, r, k)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mrunning_mean_update\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mone_hot_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlerp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunning_mean_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected dtype float for `end` but got dtype c10::BFloat16"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pre-Trained Model Import"
      ],
      "metadata": {
        "id": "f-YwalWDUyUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_channels: int = 32,\n",
        "        num_quantizers: int = 8,\n",
        "        num_embeddings: int = 1024,\n",
        "        padding: str = \"same\"\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(n_channels, padding)\n",
        "        self.decoder = Decoder(n_channels, padding)\n",
        "        self.quantizer = ResidualVectorQuantizer(\n",
        "            num_quantizers, num_embeddings, n_channels * 16)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encode(x)\n",
        "\n",
        "\n",
        "    def encode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        assert input.ndim == 2\n",
        "        x = torch.unsqueeze(input, 1)\n",
        "        x = self.encoder(x)\n",
        "        x = torch.transpose(x, -1, -2)\n",
        "        print(x.shape)\n",
        "        _, codes, _ = self.quantizer(x)\n",
        "        return codes\n",
        "\n",
        "    def decode(self, input: torch.Tensor) -> torch.Tensor:\n",
        "        # input: [batch_size, length, num_quantizers]\n",
        "        x = self.quantizer.dequantize(input)\n",
        "        x = torch.transpose(x, -1, -2)\n",
        "        x = self.decoder(x)\n",
        "        x = torch.squeeze(x, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def soundstream_16khz(pretrained=False, **kwargs):\n",
        "    \"\"\"SoundStream encoder decoder\n",
        "\n",
        "    pretrained (bool): kwargs, load pretrained weights into the model\n",
        "    \"\"\"\n",
        "    # Call the model, load pretrained weights\n",
        "    model = EncoderDecoder()\n",
        "    state_dict = torch.hub.load_state_dict_from_url(\"https://github.com/kaiidams/soundstream-pytorch/releases/download/v1.0/soundstream_16khz-20230425.ckpt\", map_location='cpu')\n",
        "    model.load_state_dict(state_dict['state_dict'], strict=False)\n",
        "    model.eval()\n",
        "    return model"
      ],
      "metadata": {
        "id": "E_GAycevkPy5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torchaudio\n",
        "#import torch\n",
        "\n",
        "#model = torch.hub.load(\"kaiidams/soundstream-pytorch\", \"soundstream_16khz\")\n",
        "model = soundstream_16khz()\n",
        "x, sr = torchaudio.load('canterburytales_09_chaucer_64kb_0000.flac')\n",
        "x, sr = torchaudio.functional.resample(x, sr, 16000), 16000\n",
        "\n",
        "x = x[:, 0:5*16000]\n",
        "with torch.no_grad():\n",
        "    y = model.encode(x)\n",
        "    #y = y[:, :, :4]  # if you want to reduce code size.\n",
        "    z = model.decode(y)\n",
        "#torchaudio.save('output.flac', z, sr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r974uuhLcKJv",
        "outputId": "d1058b80-354e-45ac-969f-aad8937773d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 251, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torchaudio.save('output.flac', z, sr)"
      ],
      "metadata": {
        "id": "GLqL0uYLgN8i"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r, sr = torchaudio.load('canterburytales_09_chaucer_64kb_0000.flac')\n",
        "r, sr = torchaudio.functional.resample(r, sr, 16000), 16000"
      ],
      "metadata": {
        "id": "AKMmwDLGdBlr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(z.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4c0j9c2eepC",
        "outputId": "4fb24ed6-d3bf-466a-d031-6dcb9cc67bb3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 79950])\n"
          ]
        }
      ]
    }
  ]
}