{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d969b6d-c5f6-4512-8b78-da455c31205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531721cf-9b1a-4508-b273-fe9f0aec0c42",
   "metadata": {},
   "source": [
    "## 1. Creating the Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ec794d2-d86a-41ba-b429-9ecf4ec4aef9",
   "metadata": {},
   "source": [
    "from torchaudio.transforms import Resample\n",
    "from data import LibriDataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83fee74-8f20-4303-a45a-0f675e8a4fd3",
   "metadata": {},
   "source": [
    "SAMPLE_FREQ = 16000\n",
    "AUDIO_MAX_DURATION = 60\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "# Creating the dataset and dataloader\n",
    "\n",
    "trainDataset = LibriDataset(\"data_cut\", newSampleFreq = SAMPLE_FREQ, maxLenght = SAMPLE_FREQ * AUDIO_MAX_DURATION)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5af4cea-6b17-4859-a09c-6fc37ec42540",
   "metadata": {},
   "source": [
    "print(\"Audio in the dataset: \" + str(len(trainDataset)))\n",
    "print(\"Frame per item: \" + str(trainDataset[2][0].shape))\n",
    "Audio(trainDataset[40][0], rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b93c2-1d22-4982-aab2-562274398151",
   "metadata": {},
   "source": [
    "### 2. HuBERT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635a0dbe-037b-470f-b01c-5887976860f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import HubertModel,AutoProcessor,AutoFeatureExtractor,Wav2Vec2Processor,HubertForCTC\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "from hubertKM import SemanticTokenizer, visualizeEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a10e9-11a5-4601-b3ba-70eb184dd05c",
   "metadata": {},
   "source": [
    "#### Importing the pretrained models for HuBERT and KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c8472c-751a-4ff1-9d61-563c36a98790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\fabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# A semantic tokenizer\n",
    "# Input: (w2vCheckpointPath, kmeansCheckpointPath)\n",
    "# Output: (semanticTokens, normalizedEmbeddings)\n",
    "w2vBERT = SemanticTokenizer(\"facebook/hubert-base-ls960\",\"./hubertKM/hubert_base_ls960_L9_km500.bin\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1684140-30bb-42f2-946d-52ac096687d6",
   "metadata": {},
   "source": [
    "#### Computing semantic tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c07ccfc-c2e2-4ea4-be30-5f920ef898e8",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    s = time.time()\n",
    "    ##semanticTokens, embeddings = w2vBERT(trainDataset[0])\n",
    "    #semanticTokens, embeddings = w2vBERT(next(iter(trainDataLoader)).squeeze())\n",
    "    print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd526997-2dff-4ae2-a41e-8a8614df9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semanticTokens, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ce5db-e321-489d-9b96-0c4bcf9376eb",
   "metadata": {},
   "source": [
    "#### Visualizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e177c2c7-b8a4-445c-a09f-1f34d824d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some plots using PCA and t-SNE\n",
    "\n",
    "# visualizeEmbeddings(embeddings, semanticTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff33d1b-c960-460b-9bb7-8e93c0a15df2",
   "metadata": {},
   "source": [
    "#### Check with another implementation (OPTIONAL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "715088b1-2e62-47d1-80ac-f7a0dfef4551",
   "metadata": {},
   "source": [
    "# Models from https://github.com/lucidrains/audiolm-pytorch\n",
    "\n",
    "!pip install audiolm-pytorch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d561d1f-f1a4-4ab1-904a-efe839265162",
   "metadata": {},
   "source": [
    "from audiolm_pytorch import HubertWithKmeans\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = './hubertKM/hubert_base_ls960.pt',\n",
    "    kmeans_path = './hubertKM/hubert_base_ls960_L9_km500.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15ef3d89-a05a-4475-82d5-02ff82b5245c",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    s = time.time()\n",
    "    ##semanticTokens = w2vBERT(trainDataset[1])\n",
    "    s2 = time.time()\n",
    "    #semanticTokensAlt = wav2vec(next(iter(trainDataLoader)).squeeze())\n",
    "    s3 = time.time()\n",
    "\n",
    "    print(s3-s2,s2-s)\n",
    "    \n",
    "# Test if tokens are equal\n",
    "#tokensEqualityTest = not (semanticTokensAlt.squeeze() - semanticTokens).any()\n",
    "#tokensEqualityTest 6.852179765701294 7.743693113327026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bede15-a67d-4b38-a185-f9e85473598b",
   "metadata": {},
   "source": [
    "## 3. Creating the Semantic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f784d68d-8afe-4968-800c-a96c628e29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoundStream import soundstream_16khz, audio_to_tokens, tokens_to_audio, encode_audio, decode_audio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe2ad079-ab89-4176-8da4-b7e3476fb04d",
   "metadata": {},
   "source": [
    "Audio(trainDataset[40][0], rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64f910b-93b3-4abc-b1c5-9eb1ed6c8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundStream = soundstream_16khz()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08227788-64dd-4a3c-9909-8cf3c2c6485e",
   "metadata": {},
   "source": [
    "audioWave, sampleRate = torchaudio.load(\"data_cut\\\\16\\\\352\\\\little_lame_prince_01_64kb_0000.flac\")\n",
    "\n",
    "x = encode_audio(audioWave, sampleRate, soundStream)\n",
    "print(x.shape)\n",
    "Audio(audioWave, rate = sampleRate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb41d156-9384-4b0d-b5c7-7ee0a52375d7",
   "metadata": {},
   "source": [
    "x = soundStream.encode(audioWave)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b444731-3eb4-4e34-b6fb-252feb95e326",
   "metadata": {},
   "source": [
    "# x = soundStream.encode(next(iter(trainDataLoader)).squeeze())\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e75da434-f1aa-4b5a-b2a1-920cfe8cd13d",
   "metadata": {},
   "source": [
    "coarse, fine = audio_to_tokens(audioWave, sampleRate, soundStream)\n",
    "print(coarse.shape, fine.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36665ad3-1dc6-4f2b-b425-608148c2d012",
   "metadata": {},
   "source": [
    "y = decode_audio(x, soundStream)\n",
    "print(x)\n",
    "Audio(y, rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f19546-7dce-4dff-ae24-5cc95627e69e",
   "metadata": {},
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da3cedff-5c31-463c-b8e3-874dad36d607",
   "metadata": {},
   "source": [
    "y2 = tokens_to_audio(coarse, fine, soundStream)\n",
    "Audio(y2, rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15243c0d-d248-430c-abbe-6ae2d00f3de9",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8645e6c7-2eb6-46c4-9080-3325f3a7bef6",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8c804da-5b7d-47b5-af6a-171fed52a19e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def storeTokens(audioDir, outDir, w2vBERT, soundStream, fileCountCheckpoint = 5):\n",
    "\n",
    "    Path(outDir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    isNewFile = not os.path.exists(os.path.join(outDir, \"out.csv\"))\n",
    "\n",
    "    ## Check for eventual checkpoints\n",
    "    fileChecked = 0\n",
    "    reachedCheckpoint = False\n",
    "    lastFile = None\n",
    "    \n",
    "    if os.path.exists(os.path.join(outDir, \"checkpoint.txt\")):\n",
    "        with open(os.path.join(outDir, \"checkpoint.txt\"), mode='r', newline='') as checkpointFile:\n",
    "            \n",
    "            fileChecked, lastFile = checkpointFile.readline().strip().split(\" \")\n",
    "            fileChecked = int(fileChecked)\n",
    "            print(\"Found a checkpoint!\")\n",
    "\n",
    "    tokenData = []\n",
    "    fileCount = 0\n",
    "\n",
    "    totalFiles = 0\n",
    "    for root, dirs, files in os.walk(audioDir):\n",
    "        totalFiles += len(files)\n",
    "\n",
    "    with tqdm(total=totalFiles, desc='Processing files') as pbar:\n",
    "        for root, dirs, files in os.walk(audioDir):\n",
    "            for file in files:\n",
    "    \n",
    "                reachedCheckpoint = (fileChecked == 0 or file == lastFile or reachedCheckpoint)\n",
    "                \n",
    "                if file.endswith(\".flac\") and reachedCheckpoint and file != lastFile:\n",
    "              \n",
    "                    file_path = os.path.join(root, file)\n",
    "                    waveform, sr = torchaudio.load(file_path)\n",
    "                    with torch.no_grad():\n",
    "                        semanticTokens, _ = w2vBERT(waveform)\n",
    "                        coarseTokens, fineTokens = audio_to_tokens(waveform, sr, soundStream)\n",
    "                        \n",
    "                    tokenData.append([file, semanticTokens.tolist(), coarseTokens.tolist(), fineTokens.tolist()])\n",
    "    \n",
    "                    fileCount += 1\n",
    "    \n",
    "                if fileCount % fileCountCheckpoint == 0 and reachedCheckpoint and file != lastFile:\n",
    "                    with open(os.path.join(outDir, \"out.csv\"), mode='a', newline='') as outFile, open(os.path.join(outDir, \"checkpoint.txt\"), mode='w', newline='') as checkpointFile:\n",
    "                        writer = csv.writer(outFile, delimiter = \";\")\n",
    "    \n",
    "                        ## Add header in case of newFile\n",
    "                        if isNewFile:\n",
    "                            outFile.write(\"sep=;\\n\")\n",
    "                            writer.writerow([\"fileName\", \"semanticTokens\", \"coarseTokens\", \"fineTokens\"])\n",
    "                            isNewFile = not isNewFile\n",
    "                            \n",
    "                        writer.writerows(tokenData)\n",
    "    \n",
    "                        checkpointFile.write(f\"{fileCount + fileChecked} {file}\")\n",
    "                        \n",
    "                    print(f\"SAVED {fileCount} AUDIO ON OUTPUT {os.path.join(outDir, 'out.csv')}. Total of {fileCount + fileChecked} records saved.\") \n",
    "                    tokenData = []\n",
    "                    \n",
    "                pbar.update(1) \n",
    "\n",
    "    return fileCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363d987c-7c31-47c6-8b1d-0ab854622237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data import storeTokens,  TokensDataset\n",
    "\n",
    "tokenPath = \"out\"\n",
    "tokenFile = \"out.csv\"\n",
    "audioPath = \"data_cut\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccb6662-f99c-4183-a70e-2f254a2f44f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fileCount = storeTokens(audioPath, tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c254dc63-ffc3-4da7-ae8b-116b4617c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 30, includeSemanticTokens = True, includeCoarseTokens = True, includeFineTokens = True) \n",
    "#semanticDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 30, includeSemanticTokens = True)\n",
    "coarseDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 10, includeCoarseTokens = True)\n",
    "#fineDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 30, includeFineTokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95387c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from TransformerModel import Decoder\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f4e7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#dataset = semanticDataset\n",
    "train_dataset = coarseDataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "input_size = 0\n",
    "for batch in train_loader:\n",
    "    input, labels = batch\n",
    "    print(input.requires_grad)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ee5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Change vocab_size and seq_len to initiate\n",
    "model = Decoder(vocab_size=10000, seq_len=1503)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "713d2a52",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Callbacks for saving the best model and early stopping\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    dirpath='checkpoints',\n",
    "    filename='best-checkpoint',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Trainer configuration\n",
    "trainer = pl.Trainer(\n",
    "\tcallbacks=[checkpoint_callback],\n",
    "    max_epochs=10,  # Set the number of epochs\n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',  # Use GPU if available\n",
    "    devices=1 if torch.cuda.is_available() else 1,  # Number of devices for both GPU and CPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a90b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 55/55 [3:34:32<00:00, 234.05s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 7.94355732310902\n",
      "Checkpoint saved at ./checkpoints\\checkpoint_epoch_1.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 5/55 [18:08<2:57:12, 212.65s/it]"
     ]
    }
   ],
   "source": [
    "model.train(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc2360cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\c'\n",
      "<>:3: SyntaxWarning: invalid escape sequence '\\c'\n",
      "C:\\Users\\fabri\\AppData\\Local\\Temp\\ipykernel_15548\\3471269155.py:3: SyntaxWarning: invalid escape sequence '\\c'\n",
      "  checkpoint = torch.load(\"checkpoints\\checkpoint_epoch_1.pt\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = Decoder.load_from_checkpoint(checkpoint_path=\"checkpoints\\checkpoint_epoch_1.pt\")\n",
    "\n",
    "checkpoint = torch.load(\"checkpoints\\checkpoint_epoch_1.pt\")\n",
    "model = Decoder(vocab_size=10000, seq_len=1503)  # Initialize your model\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bb44b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 516, 1284, 2228,  ...,  962, 1127, 2489]]), tensor([[1284, 2228,  605,  ..., 1127, 2489, -100]]))\n"
     ]
    }
   ],
   "source": [
    "#model.eval()\n",
    "x_input = coarseDataset.__getitem__(0)\n",
    "\n",
    "print(x_input)\n",
    "with torch.no_grad():\n",
    "\tpredictions = model(x_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19ad940f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1503])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = torch.argmax(predictions, dim=-1)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4fb4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 10, includeFineTokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e21798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2505])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_input = fineDataset.__getitem__(0)[0]\n",
    "\n",
    "y_input = y_input.squeeze(0)\n",
    "pred_new = pred.squeeze(0)\n",
    "#waveform = tokens_to_audio(pred_new, y_input, soundStream)\n",
    "y_input.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
