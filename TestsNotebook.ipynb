{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d969b6d-c5f6-4512-8b78-da455c31205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531721cf-9b1a-4508-b273-fe9f0aec0c42",
   "metadata": {},
   "source": [
    "## 1. Creating the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671d0ef5-968d-49e1-9690-3329ec5f608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Resample\n",
    "from data import LibriDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e06ee-493f-497b-98b8-84080ba83857",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_FREQ = 16000\n",
    "AUDIO_MAX_DURATION = 60\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "# Creating the dataset and dataloader\n",
    "\n",
    "trainDataset = LibriDataset(\"data_cut\", newSampleFreq = SAMPLE_FREQ, maxLenght = SAMPLE_FREQ * AUDIO_MAX_DURATION)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b544acfd-c162-46f4-b49a-43904e97549f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Audio in the dataset: \" + str(len(trainDataset)))\n",
    "print(\"Frame per item: \" + str(trainDataset[2][0].shape))\n",
    "Audio(trainDataset[40][0], rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b93c2-1d22-4982-aab2-562274398151",
   "metadata": {},
   "source": [
    "### 2. HuBERT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a0dbe-037b-470f-b01c-5887976860f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HubertModel,AutoProcessor,AutoFeatureExtractor,Wav2Vec2Processor,HubertForCTC\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "from hubertKM import SemanticTokenizer, visualizeEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a10e9-11a5-4601-b3ba-70eb184dd05c",
   "metadata": {},
   "source": [
    "#### Importing the pretrained models for HuBERT and KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8472c-751a-4ff1-9d61-563c36a98790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A semantic tokenizer\n",
    "# Input: (w2vCheckpointPath, kmeansCheckpointPath)\n",
    "# Output: (semanticTokens, normalizedEmbeddings)\n",
    "w2vBERT = SemanticTokenizer(\"facebook/hubert-base-ls960\",\"./hubertKM/hubert_base_ls960_L9_km500.bin\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1684140-30bb-42f2-946d-52ac096687d6",
   "metadata": {},
   "source": [
    "#### Computing semantic tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b7236b-23b8-4508-970d-6abe6ea9d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    s = time.time()\n",
    "    ##semanticTokens, embeddings = w2vBERT(trainDataset[0])\n",
    "    #semanticTokens, embeddings = w2vBERT(next(iter(trainDataLoader)).squeeze())\n",
    "    print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd526997-2dff-4ae2-a41e-8a8614df9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semanticTokens, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ce5db-e321-489d-9b96-0c4bcf9376eb",
   "metadata": {},
   "source": [
    "#### Visualizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e177c2c7-b8a4-445c-a09f-1f34d824d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some plots using PCA and t-SNE\n",
    "\n",
    "# visualizeEmbeddings(embeddings, semanticTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff33d1b-c960-460b-9bb7-8e93c0a15df2",
   "metadata": {},
   "source": [
    "#### Check with another implementation (OPTIONAL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "715088b1-2e62-47d1-80ac-f7a0dfef4551",
   "metadata": {},
   "source": [
    "# Models from https://github.com/lucidrains/audiolm-pytorch\n",
    "\n",
    "!pip install audiolm-pytorch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d561d1f-f1a4-4ab1-904a-efe839265162",
   "metadata": {},
   "source": [
    "from audiolm_pytorch import HubertWithKmeans\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = './hubertKM/hubert_base_ls960.pt',\n",
    "    kmeans_path = './hubertKM/hubert_base_ls960_L9_km500.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15ef3d89-a05a-4475-82d5-02ff82b5245c",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    s = time.time()\n",
    "    ##semanticTokens = w2vBERT(trainDataset[1])\n",
    "    s2 = time.time()\n",
    "    #semanticTokensAlt = wav2vec(next(iter(trainDataLoader)).squeeze())\n",
    "    s3 = time.time()\n",
    "\n",
    "    print(s3-s2,s2-s)\n",
    "    \n",
    "# Test if tokens are equal\n",
    "#tokensEqualityTest = not (semanticTokensAlt.squeeze() - semanticTokens).any()\n",
    "#tokensEqualityTest 6.852179765701294 7.743693113327026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bede15-a67d-4b38-a185-f9e85473598b",
   "metadata": {},
   "source": [
    "## 3. Creating the Semantic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f784d68d-8afe-4968-800c-a96c628e29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoundStream import soundstream_16khz, audio_to_tokens, tokens_to_audio, encode_audio, decode_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e00c1-7c71-445e-9080-490ad7637beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(trainDataset[40][0], rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64f910b-93b3-4abc-b1c5-9eb1ed6c8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundStream = soundstream_16khz()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf67169c-1c31-4f15-a508-eb1996bb3600",
   "metadata": {},
   "source": [
    "audioWave, sampleRate = torchaudio.load(\"data_cut\\\\16\\\\352\\\\little_lame_prince_01_64kb_0000.flac\")\n",
    "\n",
    "x = encode_audio(audioWave, sampleRate, soundStream)\n",
    "print(x.shape)\n",
    "Audio(audioWave, rate = sampleRate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb41d156-9384-4b0d-b5c7-7ee0a52375d7",
   "metadata": {},
   "source": [
    "x = soundStream.encode(audioWave)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b444731-3eb4-4e34-b6fb-252feb95e326",
   "metadata": {},
   "source": [
    "# x = soundStream.encode(next(iter(trainDataLoader)).squeeze())\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e75da434-f1aa-4b5a-b2a1-920cfe8cd13d",
   "metadata": {},
   "source": [
    "coarse, fine = audio_to_tokens(audioWave, sampleRate, soundStream)\n",
    "print(coarse.shape, fine.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36665ad3-1dc6-4f2b-b425-608148c2d012",
   "metadata": {},
   "source": [
    "y = decode_audio(x, soundStream)\n",
    "print(x)\n",
    "Audio(y, rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f19546-7dce-4dff-ae24-5cc95627e69e",
   "metadata": {},
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da3cedff-5c31-463c-b8e3-874dad36d607",
   "metadata": {},
   "source": [
    "y2 = tokens_to_audio(coarse, fine, soundStream)\n",
    "Audio(y2, rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15243c0d-d248-430c-abbe-6ae2d00f3de9",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8645e6c7-2eb6-46c4-9080-3325f3a7bef6",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8c804da-5b7d-47b5-af6a-171fed52a19e",
   "metadata": {},
   "source": [
    "def storeTokens(audioDir, outDir, w2vBERT, soundStream, fileCountCheckpoint = 5):\n",
    "\n",
    "    Path(outDir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    isNewFile = not os.path.exists(os.path.join(outDir, \"out.csv\"))\n",
    "\n",
    "    ## Check for eventual checkpoints\n",
    "    fileChecked = 0\n",
    "    reachedCheckpoint = False\n",
    "    lastFile = None\n",
    "    \n",
    "    if os.path.exists(os.path.join(outDir, \"checkpoint.txt\")):\n",
    "        with open(os.path.join(outDir, \"checkpoint.txt\"), mode='r', newline='') as checkpointFile:\n",
    "            \n",
    "            fileChecked, lastFile = checkpointFile.readline().strip().split(\" \")\n",
    "            fileChecked = int(fileChecked)\n",
    "            print(\"Found a checkpoint!\")\n",
    "\n",
    "    tokenData = []\n",
    "    fileCount = 0\n",
    "\n",
    "    totalFiles = 0\n",
    "    for root, dirs, files in os.walk(audioDir):\n",
    "        totalFiles += len(files)\n",
    "\n",
    "    with tqdm(total=totalFiles, desc='Processing files') as pbar:\n",
    "        for root, dirs, files in os.walk(audioDir):\n",
    "            for file in files:\n",
    "    \n",
    "                reachedCheckpoint = (fileChecked == 0 or file == lastFile or reachedCheckpoint)\n",
    "                \n",
    "                if file.endswith(\".flac\") and reachedCheckpoint and file != lastFile:\n",
    "              \n",
    "                    file_path = os.path.join(root, file)\n",
    "                    waveform, sr = torchaudio.load(file_path)\n",
    "                    with torch.no_grad():\n",
    "                        semanticTokens, _ = w2vBERT(waveform)\n",
    "                        coarseTokens, fineTokens = audio_to_tokens(waveform, sr, soundStream)\n",
    "                        \n",
    "                    tokenData.append([file, semanticTokens.tolist(), coarseTokens.tolist(), fineTokens.tolist()])\n",
    "    \n",
    "                    fileCount += 1\n",
    "    \n",
    "                if fileCount % fileCountCheckpoint == 0 and reachedCheckpoint and file != lastFile:\n",
    "                    with open(os.path.join(outDir, \"out.csv\"), mode='a', newline='') as outFile, open(os.path.join(outDir, \"checkpoint.txt\"), mode='w', newline='') as checkpointFile:\n",
    "                        writer = csv.writer(outFile, delimiter = \";\")\n",
    "    \n",
    "                        ## Add header in case of newFile\n",
    "                        if isNewFile:\n",
    "                            outFile.write(\"sep=;\\n\")\n",
    "                            writer.writerow([\"fileName\", \"semanticTokens\", \"coarseTokens\", \"fineTokens\"])\n",
    "                            isNewFile = not isNewFile\n",
    "                            \n",
    "                        writer.writerows(tokenData)\n",
    "    \n",
    "                        checkpointFile.write(f\"{fileCount + fileChecked} {file}\")\n",
    "                        \n",
    "                    print(f\"SAVED {fileCount} AUDIO ON OUTPUT {os.path.join(outDir, 'out.csv')}. Total of {fileCount + fileChecked} records saved.\") \n",
    "                    tokenData = []\n",
    "                    \n",
    "                pbar.update(1) \n",
    "\n",
    "    return fileCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d987c-7c31-47c6-8b1d-0ab854622237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data import storeTokens\n",
    "\n",
    "fileCount = storeTokens(\"data_cut\",\"out\", w2vBERT, soundStream, fileCountCheckpoint = 2)\n",
    "fileCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c254dc63-ffc3-4da7-ae8b-116b4617c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"out/out.csv\", mode='r', newline='') as file:\n",
    "    content = csv.reader(file, delimiter = \";\")\n",
    "    for row in content:\n",
    "        print(row[0])\n",
    "    #print(sum(1 for row in content) - 2, \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6006b9bd-f575-4e87-987b-bd1222fcccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"data_cut\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".flac\"):\n",
    "                print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb53a1e-384f-4fec-8a05-cb3fbb7ac227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
