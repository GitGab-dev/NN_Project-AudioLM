{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d86e3b5",
   "metadata": {},
   "source": [
    "## 0. Colab Code"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9d46ab8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!pip install pytorch_lightning\n",
    "\n",
    "!cd ./drive/\"My Drive\"/AudioLM\n",
    "import sys\n",
    "sys.path.append('./drive/My Drive/AudioLM')\n",
    "%cd ./drive/MyDrive/AudioLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac72ac",
   "metadata": {},
   "source": [
    "## 1. Preparing the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d969b6d-c5f6-4512-8b78-da455c31205d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531721cf-9b1a-4508-b273-fe9f0aec0c42",
   "metadata": {},
   "source": [
    "## 0.5. Creating the Dataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ec794d2-d86a-41ba-b429-9ecf4ec4aef9",
   "metadata": {},
   "source": [
    "from torchaudio.transforms import Resample\n",
    "from data import LibriDataset"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c83fee74-8f20-4303-a45a-0f675e8a4fd3",
   "metadata": {},
   "source": [
    "SAMPLE_FREQ = 16000\n",
    "AUDIO_MAX_DURATION = 60\n",
    "BATCH_SIZE = 12\n",
    "\n",
    "# Creating the dataset and dataloader\n",
    "\n",
    "trainDataset = LibriDataset(\"data_cut\", newSampleFreq = SAMPLE_FREQ, maxLenght = SAMPLE_FREQ * AUDIO_MAX_DURATION)\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5af4cea-6b17-4859-a09c-6fc37ec42540",
   "metadata": {},
   "source": [
    "print(\"Audio in the dataset: \" + str(len(trainDataset)))\n",
    "print(\"Frame per item: \" + str(trainDataset[2][0].shape))\n",
    "Audio(trainDataset[40][0], rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b93c2-1d22-4982-aab2-562274398151",
   "metadata": {},
   "source": [
    "## 2. HuBERT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635a0dbe-037b-470f-b01c-5887976860f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import HubertModel,AutoProcessor,AutoFeatureExtractor,Wav2Vec2Processor,HubertForCTC\n",
    "import joblib\n",
    "import torch.nn as nn\n",
    "from hubertKM import SemanticTokenizer, visualizeEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08a10e9-11a5-4601-b3ba-70eb184dd05c",
   "metadata": {},
   "source": [
    "#### Importing the pretrained models for HuBERT and KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c8472c-751a-4ff1-9d61-563c36a98790",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/hubert-base-ls960 were not used when initializing HubertModel: ['encoder.pos_conv_embed.conv.weight_g', 'encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing HubertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing HubertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of HubertModel were not initialized from the model checkpoint at facebook/hubert-base-ls960 and are newly initialized: ['encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\fabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MiniBatchKMeans from version 0.24.0 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# A semantic tokenizer\n",
    "# Input: (w2vCheckpointPath, kmeansCheckpointPath)\n",
    "# Output: (semanticTokens, normalizedEmbeddings)\n",
    "w2vBERT = SemanticTokenizer(\"facebook/hubert-base-ls960\",\"./hubertKM/hubert_base_ls960_L9_km500.bin\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1684140-30bb-42f2-946d-52ac096687d6",
   "metadata": {},
   "source": [
    "#### Computing semantic tokens"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c07ccfc-c2e2-4ea4-be30-5f920ef898e8",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    s = time.time()\n",
    "    ##semanticTokens, embeddings = w2vBERT(trainDataset[0])\n",
    "    #semanticTokens, embeddings = w2vBERT(next(iter(trainDataLoader)).squeeze())\n",
    "    print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd526997-2dff-4ae2-a41e-8a8614df9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#semanticTokens, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ce5db-e321-489d-9b96-0c4bcf9376eb",
   "metadata": {},
   "source": [
    "#### Visualizing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e177c2c7-b8a4-445c-a09f-1f34d824d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some plots using PCA and t-SNE\n",
    "\n",
    "# visualizeEmbeddings(embeddings, semanticTokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff33d1b-c960-460b-9bb7-8e93c0a15df2",
   "metadata": {},
   "source": [
    "#### Check with another implementation (OPTIONAL)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "715088b1-2e62-47d1-80ac-f7a0dfef4551",
   "metadata": {},
   "source": [
    "# Models from https://github.com/lucidrains/audiolm-pytorch\n",
    "\n",
    "!pip install audiolm-pytorch"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d561d1f-f1a4-4ab1-904a-efe839265162",
   "metadata": {},
   "source": [
    "from audiolm_pytorch import HubertWithKmeans\n",
    "\n",
    "wav2vec = HubertWithKmeans(\n",
    "    checkpoint_path = './hubertKM/hubert_base_ls960.pt',\n",
    "    kmeans_path = './hubertKM/hubert_base_ls960_L9_km500.bin'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15ef3d89-a05a-4475-82d5-02ff82b5245c",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    s = time.time()\n",
    "    ##semanticTokens = w2vBERT(trainDataset[1])\n",
    "    s2 = time.time()\n",
    "    #semanticTokensAlt = wav2vec(next(iter(trainDataLoader)).squeeze())\n",
    "    s3 = time.time()\n",
    "\n",
    "    print(s3-s2,s2-s)\n",
    "    \n",
    "# Test if tokens are equal\n",
    "#tokensEqualityTest = not (semanticTokensAlt.squeeze() - semanticTokens).any()\n",
    "#tokensEqualityTest 6.852179765701294 7.743693113327026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bede15-a67d-4b38-a185-f9e85473598b",
   "metadata": {},
   "source": [
    "## 3. Creating the Semantic Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f784d68d-8afe-4968-800c-a96c628e29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SoundStream import soundstream_16khz, audio_to_tokens, tokens_to_audio, encode_audio, decode_audio"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe2ad079-ab89-4176-8da4-b7e3476fb04d",
   "metadata": {},
   "source": [
    "Audio(trainDataset[40][0], rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64f910b-93b3-4abc-b1c5-9eb1ed6c8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "soundStream = soundstream_16khz()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "08227788-64dd-4a3c-9909-8cf3c2c6485e",
   "metadata": {},
   "source": [
    "audioWave, sampleRate = torchaudio.load(\"data_cut\\\\16\\\\352\\\\little_lame_prince_01_64kb_0000.flac\")\n",
    "\n",
    "x = encode_audio(audioWave, sampleRate, soundStream)\n",
    "print(x.shape)\n",
    "Audio(audioWave, rate = sampleRate)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb41d156-9384-4b0d-b5c7-7ee0a52375d7",
   "metadata": {},
   "source": [
    "x = soundStream.encode(audioWave)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b444731-3eb4-4e34-b6fb-252feb95e326",
   "metadata": {},
   "source": [
    "# x = soundStream.encode(next(iter(trainDataLoader)).squeeze())\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e75da434-f1aa-4b5a-b2a1-920cfe8cd13d",
   "metadata": {},
   "source": [
    "coarse, fine = audio_to_tokens(audioWave, sampleRate, soundStream)\n",
    "print(coarse.shape, fine.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36665ad3-1dc6-4f2b-b425-608148c2d012",
   "metadata": {},
   "source": [
    "y = decode_audio(x, soundStream)\n",
    "print(x)\n",
    "Audio(y, rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70f19546-7dce-4dff-ae24-5cc95627e69e",
   "metadata": {},
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da3cedff-5c31-463c-b8e3-874dad36d607",
   "metadata": {},
   "source": [
    "y2 = tokens_to_audio(coarse, fine, soundStream)\n",
    "Audio(y2, rate = SAMPLE_FREQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15243c0d-d248-430c-abbe-6ae2d00f3de9",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8645e6c7-2eb6-46c4-9080-3325f3a7bef6",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8c804da-5b7d-47b5-af6a-171fed52a19e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def storeTokens(audioDir, outDir, w2vBERT, soundStream, fileCountCheckpoint = 5):\n",
    "\n",
    "    Path(outDir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    isNewFile = not os.path.exists(os.path.join(outDir, \"out.csv\"))\n",
    "\n",
    "    ## Check for eventual checkpoints\n",
    "    fileChecked = 0\n",
    "    reachedCheckpoint = False\n",
    "    lastFile = None\n",
    "    \n",
    "    if os.path.exists(os.path.join(outDir, \"checkpoint.txt\")):\n",
    "        with open(os.path.join(outDir, \"checkpoint.txt\"), mode='r', newline='') as checkpointFile:\n",
    "            \n",
    "            fileChecked, lastFile = checkpointFile.readline().strip().split(\" \")\n",
    "            fileChecked = int(fileChecked)\n",
    "            print(\"Found a checkpoint!\")\n",
    "\n",
    "    tokenData = []\n",
    "    fileCount = 0\n",
    "\n",
    "    totalFiles = 0\n",
    "    for root, dirs, files in os.walk(audioDir):\n",
    "        totalFiles += len(files)\n",
    "\n",
    "    with tqdm(total=totalFiles, desc='Processing files') as pbar:\n",
    "        for root, dirs, files in os.walk(audioDir):\n",
    "            for file in files:\n",
    "    \n",
    "                reachedCheckpoint = (fileChecked == 0 or file == lastFile or reachedCheckpoint)\n",
    "                \n",
    "                if file.endswith(\".flac\") and reachedCheckpoint and file != lastFile:\n",
    "              \n",
    "                    file_path = os.path.join(root, file)\n",
    "                    waveform, sr = torchaudio.load(file_path)\n",
    "                    with torch.no_grad():\n",
    "                        semanticTokens, _ = w2vBERT(waveform)\n",
    "                        coarseTokens, fineTokens = audio_to_tokens(waveform, sr, soundStream)\n",
    "                        \n",
    "                    tokenData.append([file, semanticTokens.tolist(), coarseTokens.tolist(), fineTokens.tolist()])\n",
    "    \n",
    "                    fileCount += 1\n",
    "    \n",
    "                if fileCount % fileCountCheckpoint == 0 and reachedCheckpoint and file != lastFile:\n",
    "                    with open(os.path.join(outDir, \"out.csv\"), mode='a', newline='') as outFile, open(os.path.join(outDir, \"checkpoint.txt\"), mode='w', newline='') as checkpointFile:\n",
    "                        writer = csv.writer(outFile, delimiter = \";\")\n",
    "    \n",
    "                        ## Add header in case of newFile\n",
    "                        if isNewFile:\n",
    "                            outFile.write(\"sep=;\\n\")\n",
    "                            writer.writerow([\"fileName\", \"semanticTokens\", \"coarseTokens\", \"fineTokens\"])\n",
    "                            isNewFile = not isNewFile\n",
    "                            \n",
    "                        writer.writerows(tokenData)\n",
    "    \n",
    "                        checkpointFile.write(f\"{fileCount + fileChecked} {file}\")\n",
    "                        \n",
    "                    print(f\"SAVED {fileCount} AUDIO ON OUTPUT {os.path.join(outDir, 'out.csv')}. Total of {fileCount + fileChecked} records saved.\") \n",
    "                    tokenData = []\n",
    "                    \n",
    "                pbar.update(1) \n",
    "\n",
    "    return fileCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363d987c-7c31-47c6-8b1d-0ab854622237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data import storeTokens,  TokensDataset, store_from_librilight\n",
    "\n",
    "tokenPath = \"out\"\n",
    "tokenFile = \"out.csv\"\n",
    "audioPath = \"data_cut\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccb6662-f99c-4183-a70e-2f254a2f44f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fileCount = store_from_librilight(tokenPath, tokenFile, w2vBERT, soundStream, fileCountCheckpoint = 10, subset = \"1h\") #More than 2700 files in the 10h, for a smaller dataset use 1h (10 min is too small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c254dc63-ffc3-4da7-ae8b-116b4617c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 30, includeSemanticTokens = True, includeCoarseTokens = True, includeFineTokens = True) \n",
    "#semanticDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 30, includeSemanticTokens = True)\n",
    "coarseDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 10, includeCoarseTokens = True)\n",
    "#fineDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 30, includeFineTokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b95387c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from TransformerModel import Decoder\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99f4e7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset, valid_dataset = random_split(coarseDataset, [0.8, 0.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=6, persistent_workers=True) #num_workers should be a fraction of the number of cores of the CPU (or GPU if in use)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=6, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5ee5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Change vocab_size and seq_len to initiate, k should be d_model/num_heads\n",
    "model = Decoder( d_model=256, num_layers = 3, num_heads=4, dim_feedforward=1024, dropout=0.1, k=64, vocab_size=10000, seq_len=1503)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "713d2a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath='checkpoints',           # Directory where checkpoints will be saved\n",
    "    filename='latest-checkpoint',    # Filename for the latest checkpoint\n",
    "    save_last=True,                  # Always save the last checkpoint\n",
    "    save_top_k=1,                    # Save the best model according to monitor\n",
    "    monitor='val_loss',              # Metric to monitor (optional if you just want the last checkpoint)\n",
    "    mode='min',                      # 'min' for loss, 'max' for accuracy, etc.\n",
    "    verbose=True,                     # Print messages when checkpoints are saved\n",
    "\tevery_n_epochs=1\n",
    ")\n",
    "\n",
    "# Trainer configuration\n",
    "trainer = pl.Trainer(\n",
    "    callbacks=[checkpoint_callback],\n",
    "    max_epochs=10, \n",
    "    accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
    "    devices=1,\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d25a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:652: Checkpoint directory C:\\Users\\fabri\\Documents\\GitHub\\NN_Project-AudioLM\\checkpoints exists and is not empty.\n",
      "Restoring states from the checkpoint path at checkpoints/manual-checkpoint.ckpt\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | embedding | Embedding        | 2.6 M  | train\n",
      "1 | layers    | ModuleList       | 2.4 M  | train\n",
      "2 | norm      | LayerNorm        | 512    | train\n",
      "3 | linear    | Linear           | 2.6 M  | train\n",
      "4 | loss_fn   | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "7.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 M     Total params\n",
      "30.194    Total estimated model params size (MB)\n",
      "Restored all states from the checkpoint at checkpoints/manual-checkpoint.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:  71%|███████▏  | 80/112 [32:59<13:11,  0.04it/s, v_num=15] "
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=valid_loader, ckpt_path=\"checkpoints/latest-checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fda8f3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"checkpoints/manual-checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb44b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.eval()\n",
    "x_input = coarseDataset.__getitem__(0)\n",
    "\n",
    "print(x_input)\n",
    "with torch.no_grad():\n",
    "\tpredictions = model(x_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.argmax(predictions, dim=-1)\n",
    "for elem in pred[0]:\n",
    "\tprint(elem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "fineDataset = TokensDataset(tokenPath, tokenFile, requiredDuration = 10, includeFineTokens = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_input = fineDataset.__getitem__(0)[0]\n",
    "\n",
    "y_input = y_input.squeeze(0)\n",
    "pred_new = pred.squeeze(0)\n",
    "#waveform = tokens_to_audio(pred_new, y_input, soundStream)\n",
    "y_input.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
